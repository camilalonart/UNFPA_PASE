{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilalonart/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-779-5055f2920167>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-779-5055f2920167>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    ,'tarde','dicho','ahorita','pasa','obviamente','robledo','afectar','pesar','semana','bajar','sale','ninguna','simple','altamente','diferente','margen','comunicación','temas','empresa','derecho','hermano','familiar','constantemente','demasiados','cultura','seguro','mantenimiento''\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import argparse\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "import spacy\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getData():\n",
    "    df = pd.read_json(r'../server/data/MedellinCleaned.json')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def preprocess(df):\n",
    "    #Remover filas duplicadas\n",
    "    df = df.drop_duplicates(subset=['respuesta'])\n",
    "\n",
    "    #remover espacios extra entre palabras\n",
    "    df['respuesta'] = df['respuesta'].str.replace('  ',' ')\n",
    "    df['respuesta'] = df['respuesta'].str.strip()\n",
    "    #transformar a minuscula\n",
    "    df['respuesta'] = df['respuesta'].str.lower()\n",
    "    #eliminar puntuacion\n",
    "    df['respuesta'] = df['respuesta'].str.replace(r'\\s+', ' ')\n",
    "    df['respuesta'] = df['respuesta'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "    #ods de ods_n como string a n entero\n",
    "    df.ods = [ int(data[data.find('_')+1:])for data in df.ods ]\n",
    "    #meta de meta_ods_n como string a n entero\n",
    "    df.meta = [ data[data.rfind('_')+1:] for data in df.meta ]\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def process(df):\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    stop_words.extend(['importante','saber','caso','decirlo','esperar','servicio','colombia','decirlo','completamente','tampoco','seguir','grave','situación','manera','mejorar','poner','hecho','villanueva','totalmente','general','conseguir','disfrutar','darle','mano','alto','claro','bello','buscar','viendo','zonas','tal','podría','afectando','primer','aún','mismos','sólo','digo','aunque','mal','encuentra','cuanto','diferentes','hoy','diría','allá','dejan','horas','habitantes','persona','poder','pronto','lugar','empiezan','alguien','presentando','actualmente','saben', 'cosa', 'sido','pues','así','acá','siempre','tan','sector','decir','pasar','mundo','territorio','siendo','varios','verdad','principalmente', 'partes','causa','hacia','sabe','sido','quiere','quién','saben','cosa','sido','tantos', 'salir', 'nunca','calles','visto','mayor','gran','dónde','veo','poca','dice','cada','da','ir','nadie','enfrenta','podemos','casi','menos','cuenta','haciendo','hacen','tipo','bien','digamos','primero','haciendo','cantidad','forma','lado','dar','después','últimamente','mejor','debido','basura','pico','carro','horrible','realmente','toda','consideró','quedan','siempre','día','problemáticas','cualquier','problemática','día','hora','genera','nivel','falta','principales','presenta','pueden','ejemplo','grande','mala','pasan','ahora','van','considero','manejan','todas','dos','parece','personas','tener','pasan','parte','creo','cuantas','segundo','veces','muchas','tema','personar','medellín','días','ciudad','barrio','gente','problemas','tiene','sector','ser','llegar','presentar','bueno','falto','generar','pues','así','acá','hace','ver','vez','si'\n",
    "    ,'generla','cierto','piso','mientras','ahí','cómo','pasando','capacidad','ninguna','tantas','toca','sectores','ven','recogen','va','debería','buenos','sabemos','ciertas','sé','necesita','tan','aquí','sino','años','pertenencia','caminar','prado','deporte','mes','mendicidad','atender','altavista','pie','dificultad','incluso'\n",
    "    ,'ve','cosas','puede','afecta','daniel','solamente','edad','barrios','atención','vivir','tolerancia','frente','municipio','comunidad','común','cuidado','vivo','buen','grandes','partir','social','difícil','vida','prueba','dominantData','sacarlo','orden','pueblo','sol','hombre','actual','imposible','intolerancia'\n",
    ",'tarde','dicho','ahorita','pasa','obviamente','robledo','afectar','pesar','semana','bajar','sale','ninguna','simple','altamente','diferente','margen','comunicación','temas','empresa','derecho','hermano','familiar','constantemente','demasiados','cultura','seguro','mantenimiento''\n",
    "\n",
    ",'debe','considera','aspectos','poquito','venir','punto','peor','responsabilidad','factor','entorno','llevar','medio','uso','tranquilo','sitio','favor','todavía','cerca','mañana','momentos','apoyo','lleva','considera','mayoría','alrededor','vereda','fácil','presentan','sacan','llegan','necesitamos','grupos','necesidad','puesto','vecino','país','camino','público','vamos','usted','dentro','ponen','segunda','zona','comunas','deben','sociedad','san_cristóbal','san_antonio','acompañamiento','tiempo','bastante','comuna','definitivamente','comuna','dando','buena','buenas','dan','secundario','afectan','mantienen','centro','existe','año','recursos','sacar','calidad','necesidades','corregimiento','malo','vemos','pase','san_cristóbal','belén','lugares','lugar','espacio','espacios','segundo','manejo','queda','tanta','demasiado','tambien','mas','segundar','problema','demás','igual','casas','problemática','entonces','hacer','mucho','quedo','mismo','momento','pienso','principal','mucha'])\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "    df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n",
    "    def sent_to_words(sentences):\n",
    "        for sent in sentences:\n",
    "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "            yield(sent)  \n",
    "\n",
    "    # Convert to list\n",
    "    data = df.values.tolist()\n",
    "    data_words = list(sent_to_words(data))\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # !python3 -m spacy download en  # run in terminal once\n",
    "    def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "        texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "        texts = [bigram_mod[doc] for doc in texts]\n",
    "        texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "        texts_out = []\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        # remove stopwords once more after lemmatization\n",
    "        texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "        return texts_out\n",
    "\n",
    "    data_ready = process_words(data_words)  # processed Text Data!\n",
    "    return data_ready, stop_words\n",
    "\n",
    "def LDAModel(df,numberOfTopics,data_ready):\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    texts = data_words_trigrams\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=numberOfTopics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True)\n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def modelPerplexityCoherenceScore(lda_model,data_words_trigrams):\n",
    "    # Compute Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return perplexity, coherence_lda\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeTopicsData(model, corpus, id2word, stop, numberOfTopics):\n",
    "    model_info = []\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    for topic, words in model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        words_dict = {}\n",
    "        for word, importance in words:\n",
    "            word_obj = {'text': word, 'value': float(importance) * 100}\n",
    "            words_dict[word] = word_obj\n",
    "        topic = {\n",
    "          'words': list(words_dict.values()),\n",
    "          'color': colors[topic],\n",
    "        }\n",
    "        model_info.append(topic)\n",
    "    json_result = json.dumps(model_info, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/result.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return model_info\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeGeneralInsight(df, odsData):\n",
    "    jsonData = {}\n",
    "    #ANIO--------------------------------------------------\n",
    "    jsonData['anio'] = [int(df.anio[0])]\n",
    "    \n",
    "    #RESPUESTAS POR PREGUNTA-----------------------------------------------\n",
    "    preguntasCount = df['pregunta'].value_counts()\n",
    "    cantidadDePreguntas = len(preguntasCount)\n",
    "    preguntasDict = {}\n",
    "    for preguntaP, cantidadP in preguntasCount.iteritems():\n",
    "        preguntasDict[preguntaP] = {'pregunta': preguntaP, 'cantidad': cantidadP}\n",
    "    preguntasListas = list(preguntasDict.values())\n",
    "    jsonData['preguntas'] = list(preguntasDict.values())\n",
    "    \n",
    "    #NUMERO DE RESPUESTAS--------------------------------------------------\n",
    "    total = 0\n",
    "    for p in preguntasListas:\n",
    "        total = max(total, p['cantidad'])\n",
    "    jsonData['totalRespuestas'] = total\n",
    "    \n",
    "    #RESPUESTAS POR EDAD-----------------------------------------------\n",
    "    edades = []\n",
    "    for edad, cantidad in df['rangoEdad'].value_counts().iteritems():\n",
    "        edades.append([edad,cantidad//cantidadDePreguntas])\n",
    "    edades.sort(key = lambda x: x[0])\n",
    "    jsonData['edad'] = edades\n",
    "    \n",
    "    #RESPUESTAS POR SEXO-----------------------------------------------\n",
    "    sexos = []\n",
    "    for genero, cantidad in df['sexo'].value_counts().iteritems():\n",
    "        sexoActual = {\"sexoNombre\": genero, \"value\": cantidad//cantidadDePreguntas}\n",
    "        sexos.append(sexoActual)\n",
    "    jsonData['sexo'] = sexos\n",
    "    \n",
    "    #RESPUESTAS POR ODS------------------------------------------------\n",
    "    \n",
    "    jsonData['porOds'] = odsData\n",
    "    #TOP DE PALABRAS----------------------------------------------------\n",
    "    topPalabras = []\n",
    "    for numero, cantidad in df['palabra'].value_counts().iteritems():\n",
    "        topPalabras.append([numero, cantidad])\n",
    "    jsonData['topPalabras'] = topPalabras[:8]\n",
    "    #CANTIDAD DE ODS POR MES--------------------------------------------\n",
    "    ''' ODS en x y una linea por mes donde y es la cantidad de ods\n",
    "        [['../images/SDGs/1.png',\n",
    "          1,\n",
    "          [1, 0],\n",
    "          [2, 0],\n",
    "          [3, 0],\n",
    "          [4, 18],\n",
    "          [5, 496],\n",
    "          [6, 234],\n",
    "          [7, 192],\n",
    "          [8, 22],\n",
    "          [9, 0],\n",
    "          [10, 0],\n",
    "          [11, 0],\n",
    "          [12, 0]],\n",
    "    '''\n",
    "    datosODS = {}\n",
    "    datosPorMes = df.groupby(\"mesTexo\")\n",
    "    for mes, datos in datosPorMes:\n",
    "        for ods, cantidad in datos['ods'].value_counts().iteritems():\n",
    "            if ods not in datosODS:\n",
    "                datosODS[ods] = {'ods':ods}\n",
    "            datosODS[ods][mes] = cantidad\n",
    "    for ods in datosODS:\n",
    "        for mes, _ in datosPorMes:\n",
    "            if mes not in datosODS[ods]:\n",
    "                datosODS[ods][mes] = 0\n",
    "\n",
    "    baseImage = '../images/SDGs/'\n",
    "    mesesTable = {\"Ene\":1,\"Feb\":2,\"Mar\":3,\"Abr\":4,\"May\":5,\"Jun\":6,\"Jul\":7,\"Aug\":8,\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dic\":12}\n",
    "    datosODS = list(datosODS.values())\n",
    "    datosODS.sort(key = lambda x: x['ods'])\n",
    "    datosEnLista = []\n",
    "    for entrada in datosODS:\n",
    "        listVersion = []\n",
    "        for key, val in entrada.items():\n",
    "            listVersion.append([key,val])\n",
    "        monthsAlreadyIn = set()\n",
    "        for i in range(1,len(listVersion)):\n",
    "            monthsAlreadyIn.add(mesesTable[listVersion[i][0]])\n",
    "            listVersion[i][0] = mesesTable[listVersion[i][0]]\n",
    "        for i in range(1,13):\n",
    "            if i not in monthsAlreadyIn:\n",
    "                listVersion.append([i,0])\n",
    "        odsS = listVersion[1:]   \n",
    "        odsS.sort(key = lambda x: x[0])\n",
    "        #listVersion[0][0] = baseImage + str(listVersion[0][1]) + '.png'\n",
    "        #listVersion = listVersion[0] + odsS\n",
    "        datosEnLista.append(listVersion)\n",
    "    jsonData['datosPorMes'] = datosEnLista\n",
    "\n",
    "    json_result = json.dumps(jsonData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/generalResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return jsonData  \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def sankeyFile(lda_model,numberOfTopics):\n",
    "    topicsData = lda_model.show_topics(num_topics = numberOfTopics,num_words=7,formatted=False)\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    for topic in topicsData:\n",
    "        currTopic = -1\n",
    "        for words in topic:\n",
    "            if type(words) == int:\n",
    "                currTopic = words\n",
    "            else:\n",
    "                for word, weight in words:\n",
    "                    sankeyRow = ['Tema '+str(currTopic+1),word,int(weight*100)]\n",
    "                    dataForSankey.append(sankeyRow)\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/sankeyResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------       \n",
    "def createDictForTopics(lda_model, numberOfTopics):\n",
    "    topicsDictWords = {}\n",
    "    i=1\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        for word, value in words:\n",
    "            listOfTopic.append(word)\n",
    "        topicsDictWords[i] = listOfTopic   \n",
    "        i+=1\n",
    "    return topicsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def createDictForOds(df):\n",
    "    odsDictWords = {}\n",
    "    for row_id, data in df.iterrows():\n",
    "        if data['ods'] not in odsDictWords:        \n",
    "            odsDictWords[data['ods']] = set()\n",
    "        if len(data['palabra'])>=3:\n",
    "            odsDictWords[data['ods']].add(data['palabra'])\n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def simmilarityOdsandTopics(topicsDictWords, odsDictWords):\n",
    "    simmilarityTopics = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        lista = []\n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            num = len(set(list1) & set(list2))\n",
    "            lista.append({'ods': odsNum,'Similaridad': num})\n",
    "        datosTopico = {'topico':'Tema'+str(topicNum), 'SimilaridadOds' : lista}\n",
    "        simmilarityTopics.append(datosTopico)\n",
    "    return simmilarityTopics\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def frequentODS(simmilarity):\n",
    "    odsTopOrder = {}\n",
    "    for topic in simmilarity:\n",
    "        for data in topic['SimilaridadOds']:\n",
    "            if data['ods'] in odsTopOrder:\n",
    "                odsTopOrder[data['ods']] += int(data['Similaridad'])\n",
    "            else:\n",
    "                odsTopOrder[data['ods']] = int(data['Similaridad'])\n",
    "    odsTopOrder = list(odsTopOrder.items())\n",
    "    odsTopOrder.sort(key = lambda x: x[1], reverse = True)\n",
    "    #change format\n",
    "    ods = []\n",
    "    for numero, cantidad in odsTopOrder:\n",
    "        ods.append([numero, cantidad])\n",
    "    return ods\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicData(lda_model,numberOfTopics,simmilarity, data_ready):\n",
    "    data_flat = [w for w_list in data_ready for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "\n",
    "    i=1\n",
    "    json_data = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        histogram = [['Palabra','Frecuencia','Importancia']]\n",
    "        infoCompleta = []\n",
    "        for word, value in words:\n",
    "            word_count = counter[word]\n",
    "            listOfTopic.append({\"text\": word,\"value\":1})\n",
    "            infoCompleta.append({\"word\":word, 'Importancia':float(value*1000), 'Frecuencia':word_count})\n",
    "            histogram.append([word,word_count, float(value*1000)])\n",
    "        lista = []\n",
    "        for t in simmilarity:\n",
    "            if t['topico']=='Tema'+str(topic+1):\n",
    "                for data in t['SimilaridadOds']:\n",
    "                    lista.append([data['ods'],data['Similaridad']])\n",
    "        lista.sort(key = lambda x: x[1], reverse = True)\n",
    "        odsRelacionado = lista[0][0]\n",
    "        odsComplementario = lista[1][0]\n",
    "        json_data.append({\"name\":\"Tema \"+str(topic+1),'infoCompleta':infoCompleta,\"words\":listOfTopic,\"ods\":odsRelacionado, \"odsComplementario\": odsComplementario,'histogram':histogram}) \n",
    "        \n",
    "        i+=1\n",
    "    print(json_data)\n",
    "    json_result = json.dumps(json_data, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/dataPerTopic.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getChord(frequency,simmilarity):\n",
    "    frequency=frequency[:6]\n",
    "    names = []\n",
    "    dataForODS = {}\n",
    "    for ods, quatity in frequency:\n",
    "        names.append(ods)\n",
    "    for data in simmilarity:\n",
    "        for row in data['SimilaridadOds']:\n",
    "            if row['ods'] in names and row['Similaridad'] > 1:\n",
    "                if row['ods'] in dataForODS:\n",
    "                    dataForODS[row['ods']].append(data['topico'])\n",
    "                else:\n",
    "                    dataForODS[row['ods']] = [data['topico']]\n",
    "    chordData = []\n",
    "    nameData = []\n",
    "    for names, temas in dataForODS.items():\n",
    "        nameData.append('ODS'+str(names))\n",
    "        array = [int(temaname[temaname.find('a')+1:]) for temaname in temas]\n",
    "        if len(array) > 8: array = array[:8]\n",
    "        if len(array) < 8: array = array + ([0]*(8-len(array)))\n",
    "        chordData.append(array)\n",
    "    json_result = json.dumps(chordData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(nameData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def swarnData(lda_model,numberOfTopics):\n",
    "    topicsDictWords = []\n",
    "    i=1\n",
    "    swarnNames = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        swarnNames.append('Tema'+str(topic+1))\n",
    "        for word, value in words:\n",
    "            topicsDictWords.append({\"group\":'Tema'+str(topic+1),\"id\":word,\"value\":int(value*100),\"volume\":int(value*100)})\n",
    "        i+=1\n",
    "    json_result = json.dumps(swarnNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(topicsDictWords, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicODSWeight(simmilarity):\n",
    "    dataBars = []\n",
    "    keysNames = set()\n",
    "    for data in simmilarity:\n",
    "        odsResults = {'Temas': data['topico']}\n",
    "        for odsDetails in data['SimilaridadOds']:\n",
    "            keysNames.add(str(odsDetails['ods']))\n",
    "            odsResults[str(odsDetails['ods'])] = int(odsDetails['Similaridad'])\n",
    "        dataBars.append(odsResults)\n",
    "    keysNames = list(keysNames)\n",
    "    json_result = json.dumps(dataBars, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentage.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(keysNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentageKeys.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def dominant():\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    doc_lens = [[' ', len(d)] for d in df_dominant_topic.Text]\n",
    "    doc_lens.insert(0,['ODS','Length'])\n",
    "    json_result = json.dumps(doc_lens, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/histogram.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return df_dominant_topic\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        print(new_doc[0])\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilalonart/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "<ipython-input-777-ed952e94183e>:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(r'\\s+', ' ')\n",
      "<ipython-input-777-ed952e94183e>:40: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace('[{}]'.format(string.punctuation), '')\n",
      "<ipython-input-777-ed952e94183e>:56: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Tema 1', 'infoCompleta': [{'word': 'salud', 'Importancia': 156.11454844474792, 'Frecuencia': 394}, {'word': 'metro', 'Importancia': 48.29804599285126, 'Frecuencia': 42}, {'word': 'plata', 'Importancia': 29.913945123553276, 'Frecuencia': 27}, {'word': 'dinero', 'Importancia': 29.853252694010735, 'Frecuencia': 69}, {'word': 'vehicular', 'Importancia': 28.823716565966606, 'Frecuencia': 62}, {'word': 'recoger', 'Importancia': 25.10751411318779, 'Frecuencia': 70}, {'word': 'autoridad', 'Importancia': 19.531913101673126, 'Frecuencia': 25}, {'word': 'adulto', 'Importancia': 18.899038434028625, 'Frecuencia': 103}, {'word': 'inclusive', 'Importancia': 14.007175341248512, 'Frecuencia': 5}, {'word': 'cultural', 'Importancia': 13.783027417957783, 'Frecuencia': 32}], 'words': [{'text': 'salud', 'value': 1}, {'text': 'metro', 'value': 1}, {'text': 'plata', 'value': 1}, {'text': 'dinero', 'value': 1}, {'text': 'vehicular', 'value': 1}, {'text': 'recoger', 'value': 1}, {'text': 'autoridad', 'value': 1}, {'text': 'adulto', 'value': 1}, {'text': 'inclusive', 'value': 1}, {'text': 'cultural', 'value': 1}], 'ods': 16, 'odsComplementario': 1, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['salud', 394, 156.11454844474792], ['metro', 42, 48.29804599285126], ['plata', 27, 29.913945123553276], ['dinero', 69, 29.853252694010735], ['vehicular', 62, 28.823716565966606], ['recoger', 70, 25.10751411318779], ['autoridad', 25, 19.531913101673126], ['adulto', 103, 18.899038434028625], ['inclusive', 5, 14.007175341248512], ['cultural', 32, 13.783027417957783]]}, {'name': 'Tema 2', 'infoCompleta': [{'word': 'transporte', 'Importancia': 130.99177181720734, 'Frecuencia': 6}, {'word': 'noche', 'Importancia': 78.45307886600494, 'Frecuencia': 57}, {'word': 'ambiente', 'Importancia': 78.31920683383942, 'Frecuencia': 3}, {'word': 'conflicto', 'Importancia': 44.21580955386162, 'Frecuencia': 118}, {'word': 'universidad', 'Importancia': 44.15467754006386, 'Frecuencia': 11}, {'word': 'estudio', 'Importancia': 38.68618607521057, 'Frecuencia': 12}, {'word': 'comercio', 'Importancia': 28.42065878212452, 'Frecuencia': 1}, {'word': 'pagar', 'Importancia': 18.480336293578148, 'Frecuencia': 43}, {'word': 'carrera', 'Importancia': 15.453382395207882, 'Frecuencia': 24}, {'word': 'muerte', 'Importancia': 12.699314393103123, 'Frecuencia': 61}], 'words': [{'text': 'transporte', 'value': 1}, {'text': 'noche', 'value': 1}, {'text': 'ambiente', 'value': 1}, {'text': 'conflicto', 'value': 1}, {'text': 'universidad', 'value': 1}, {'text': 'estudio', 'value': 1}, {'text': 'comercio', 'value': 1}, {'text': 'pagar', 'value': 1}, {'text': 'carrera', 'value': 1}, {'text': 'muerte', 'value': 1}], 'ods': 11, 'odsComplementario': 16, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['transporte', 6, 130.99177181720734], ['noche', 57, 78.45307886600494], ['ambiente', 3, 78.31920683383942], ['conflicto', 118, 44.21580955386162], ['universidad', 11, 44.15467754006386], ['estudio', 12, 38.68618607521057], ['comercio', 1, 28.42065878212452], ['pagar', 43, 18.480336293578148], ['carrera', 24, 15.453382395207882], ['muerte', 61, 12.699314393103123]]}, {'name': 'Tema 3', 'infoCompleta': [{'word': 'contaminación', 'Importancia': 328.30527424812317, 'Frecuencia': 1}, {'word': 'ambiental', 'Importancia': 75.2013698220253, 'Frecuencia': 189}, {'word': 'parque', 'Importancia': 41.85466468334198, 'Frecuencia': 90}, {'word': 'aire', 'Importancia': 33.669937402009964, 'Frecuencia': 47}, {'word': 'pobreza', 'Importancia': 33.30815210938454, 'Frecuencia': 208}, {'word': 'ruido', 'Importancia': 22.33247645199299, 'Frecuencia': 41}, {'word': 'ignorancia', 'Importancia': 16.8664138764143, 'Frecuencia': 11}, {'word': 'popo', 'Importancia': 13.743334449827671, 'Frecuencia': 30}, {'word': 'libre', 'Importancia': 10.710169561207294, 'Frecuencia': 7}, {'word': 'control', 'Importancia': 10.26640273630619, 'Frecuencia': 32}], 'words': [{'text': 'contaminación', 'value': 1}, {'text': 'ambiental', 'value': 1}, {'text': 'parque', 'value': 1}, {'text': 'aire', 'value': 1}, {'text': 'pobreza', 'value': 1}, {'text': 'ruido', 'value': 1}, {'text': 'ignorancia', 'value': 1}, {'text': 'popo', 'value': 1}, {'text': 'libre', 'value': 1}, {'text': 'control', 'value': 1}], 'ods': 11, 'odsComplementario': 1, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['contaminación', 1, 328.30527424812317], ['ambiental', 189, 75.2013698220253], ['parque', 90, 41.85466468334198], ['aire', 47, 33.669937402009964], ['pobreza', 208, 33.30815210938454], ['ruido', 41, 22.33247645199299], ['ignorancia', 11, 16.8664138764143], ['popo', 30, 13.743334449827671], ['libre', 7, 10.710169561207294], ['control', 32, 10.26640273630619]]}, {'name': 'Tema 4', 'infoCompleta': [{'word': 'inseguridad', 'Importancia': 534.100592136383, 'Frecuencia': 1354}, {'word': 'venta', 'Importancia': 54.17831987142563, 'Frecuencia': 88}, {'word': 'desigualdad', 'Importancia': 33.56928378343582, 'Frecuencia': 46}, {'word': 'robo', 'Importancia': 33.21300819516182, 'Frecuencia': 136}, {'word': 'sexual', 'Importancia': 9.888620115816593, 'Frecuencia': 27}, {'word': 'matar', 'Importancia': 8.45679547637701, 'Frecuencia': 46}, {'word': 'excremento', 'Importancia': 8.327048271894455, 'Frecuencia': 16}, {'word': 'guerra', 'Importancia': 8.14810674637556, 'Frecuencia': 27}, {'word': 'paz', 'Importancia': 7.576696574687958, 'Frecuencia': 29}, {'word': 'andar', 'Importancia': 6.926278118044138, 'Frecuencia': 54}], 'words': [{'text': 'inseguridad', 'value': 1}, {'text': 'venta', 'value': 1}, {'text': 'desigualdad', 'value': 1}, {'text': 'robo', 'value': 1}, {'text': 'sexual', 'value': 1}, {'text': 'matar', 'value': 1}, {'text': 'excremento', 'value': 1}, {'text': 'guerra', 'value': 1}, {'text': 'paz', 'value': 1}, {'text': 'andar', 'value': 1}], 'ods': 16, 'odsComplementario': 3, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['inseguridad', 1354, 534.100592136383], ['venta', 88, 54.17831987142563], ['desigualdad', 46, 33.56928378343582], ['robo', 136, 33.21300819516182], ['sexual', 27, 9.888620115816593], ['matar', 46, 8.45679547637701], ['excremento', 16, 8.327048271894455], ['guerra', 27, 8.14810674637556], ['paz', 29, 7.576696574687958], ['andar', 54, 6.926278118044138]]}, {'name': 'Tema 5', 'infoCompleta': [{'word': 'movilidad', 'Importancia': 277.4884104728699, 'Frecuencia': 494}, {'word': 'drogadicción', 'Importancia': 220.45433521270752, 'Frecuencia': 2}, {'word': 'acceso', 'Importancia': 35.98446026444435, 'Frecuencia': 99}, {'word': 'trancones', 'Importancia': 23.531420156359673, 'Frecuencia': 29}, {'word': 'hurto', 'Importancia': 16.89334399998188, 'Frecuencia': 10}, {'word': 'marihuana', 'Importancia': 12.423278763890266, 'Frecuencia': 51}, {'word': 'infraestructura', 'Importancia': 12.333148159086704, 'Frecuencia': 65}, {'word': 'convivencia', 'Importancia': 11.005296371877193, 'Frecuencia': 47}, {'word': 'hablar', 'Importancia': 8.487840183079243, 'Frecuencia': 40}, {'word': 'dormir', 'Importancia': 8.08841921389103, 'Frecuencia': 24}], 'words': [{'text': 'movilidad', 'value': 1}, {'text': 'drogadicción', 'value': 1}, {'text': 'acceso', 'value': 1}, {'text': 'trancones', 'value': 1}, {'text': 'hurto', 'value': 1}, {'text': 'marihuana', 'value': 1}, {'text': 'infraestructura', 'value': 1}, {'text': 'convivencia', 'value': 1}, {'text': 'hablar', 'value': 1}, {'text': 'dormir', 'value': 1}], 'ods': 11, 'odsComplementario': 3, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['movilidad', 494, 277.4884104728699], ['drogadicción', 2, 220.45433521270752], ['acceso', 99, 35.98446026444435], ['trancones', 29, 23.531420156359673], ['hurto', 10, 16.89334399998188], ['marihuana', 51, 12.423278763890266], ['infraestructura', 65, 12.333148159086704], ['convivencia', 47, 11.005296371877193], ['hablar', 40, 8.487840183079243], ['dormir', 24, 8.08841921389103]]}, {'name': 'Tema 6', 'infoCompleta': [{'word': 'violencia', 'Importancia': 428.1647205352783, 'Frecuencia': 1355}, {'word': 'vicio', 'Importancia': 87.44562417268753, 'Frecuencia': 39}, {'word': 'ley', 'Importancia': 23.926468566060066, 'Frecuencia': 56}, {'word': 'expendio', 'Importancia': 22.76199869811535, 'Frecuencia': 13}, {'word': 'estudiar', 'Importancia': 18.656982108950615, 'Frecuencia': 147}, {'word': 'intrafamiliar', 'Importancia': 13.428963720798492, 'Frecuencia': 24}, {'word': 'gobierno', 'Importancia': 13.186175376176834, 'Frecuencia': 2}, {'word': 'loma', 'Importancia': 11.713195592164993, 'Frecuencia': 3}, {'word': 'violento', 'Importancia': 10.6845423579216, 'Frecuencia': 7}, {'word': 'bus', 'Importancia': 9.788790717720985, 'Frecuencia': 40}], 'words': [{'text': 'violencia', 'value': 1}, {'text': 'vicio', 'value': 1}, {'text': 'ley', 'value': 1}, {'text': 'expendio', 'value': 1}, {'text': 'estudiar', 'value': 1}, {'text': 'intrafamiliar', 'value': 1}, {'text': 'gobierno', 'value': 1}, {'text': 'loma', 'value': 1}, {'text': 'violento', 'value': 1}, {'text': 'bus', 'value': 1}], 'ods': 11, 'odsComplementario': 16, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['violencia', 1355, 428.1647205352783], ['vicio', 39, 87.44562417268753], ['ley', 56, 23.926468566060066], ['expendio', 13, 22.76199869811535], ['estudiar', 147, 18.656982108950615], ['intrafamiliar', 24, 13.428963720798492], ['gobierno', 2, 13.186175376176834], ['loma', 3, 11.713195592164993], ['violento', 7, 10.6845423579216], ['bus', 40, 9.788790717720985]]}, {'name': 'Tema 7', 'infoCompleta': [{'word': 'seguridad', 'Importancia': 200.21049678325653, 'Frecuencia': 314}, {'word': 'desempleo', 'Importancia': 141.5736973285675, 'Frecuencia': 668}, {'word': 'robar', 'Importancia': 50.09040981531143, 'Frecuencia': 210}, {'word': 'miedo', 'Importancia': 48.60900342464447, 'Frecuencia': 108}, {'word': 'luz', 'Importancia': 12.552246451377869, 'Frecuencia': 17}, {'word': 'vigilancia', 'Importancia': 11.525734327733517, 'Frecuencia': 29}, {'word': 'invasión', 'Importancia': 11.053024791181087, 'Frecuencia': 4}, {'word': 'acceder', 'Importancia': 10.233929380774498, 'Frecuencia': 20}, {'word': 'local', 'Importancia': 9.836862795054913, 'Frecuencia': 14}, {'word': 'pensar', 'Importancia': 9.419121779501438, 'Frecuencia': 34}], 'words': [{'text': 'seguridad', 'value': 1}, {'text': 'desempleo', 'value': 1}, {'text': 'robar', 'value': 1}, {'text': 'miedo', 'value': 1}, {'text': 'luz', 'value': 1}, {'text': 'vigilancia', 'value': 1}, {'text': 'invasión', 'value': 1}, {'text': 'acceder', 'value': 1}, {'text': 'local', 'value': 1}, {'text': 'pensar', 'value': 1}], 'ods': 16, 'odsComplementario': 11, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['seguridad', 314, 200.21049678325653], ['desempleo', 668, 141.5736973285675], ['robar', 210, 50.09040981531143], ['miedo', 108, 48.60900342464447], ['luz', 17, 12.552246451377869], ['vigilancia', 29, 11.525734327733517], ['invasión', 4, 11.053024791181087], ['acceder', 20, 10.233929380774498], ['local', 14, 9.836862795054913], ['pensar', 34, 9.419121779501438]]}, {'name': 'Tema 8', 'infoCompleta': [{'word': 'empleo', 'Importancia': 180.1159530878067, 'Frecuencia': 9}, {'word': 'oportunidad', 'Importancia': 77.90477573871613, 'Frecuencia': 402}, {'word': 'trabajo', 'Importancia': 73.46154004335403, 'Frecuencia': 12}, {'word': 'biblioteca', 'Importancia': 47.52330482006073, 'Frecuencia': 35}, {'word': 'trabajar', 'Importancia': 29.96273525059223, 'Frecuencia': 271}, {'word': 'colegio', 'Importancia': 21.82728983461857, 'Frecuencia': 39}, {'word': 'consumir', 'Importancia': 20.501775667071342, 'Frecuencia': 297}, {'word': 'escuela', 'Importancia': 18.786609172821045, 'Frecuencia': 21}, {'word': 'rural', 'Importancia': 15.42649231851101, 'Frecuencia': 12}, {'word': 'hijo', 'Importancia': 15.404953621327877, 'Frecuencia': 79}], 'words': [{'text': 'empleo', 'value': 1}, {'text': 'oportunidad', 'value': 1}, {'text': 'trabajo', 'value': 1}, {'text': 'biblioteca', 'value': 1}, {'text': 'trabajar', 'value': 1}, {'text': 'colegio', 'value': 1}, {'text': 'consumir', 'value': 1}, {'text': 'escuela', 'value': 1}, {'text': 'rural', 'value': 1}, {'text': 'hijo', 'value': 1}], 'ods': 4, 'odsComplementario': 8, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['empleo', 9, 180.1159530878067], ['oportunidad', 402, 77.90477573871613], ['trabajo', 12, 73.46154004335403], ['biblioteca', 35, 47.52330482006073], ['trabajar', 271, 29.96273525059223], ['colegio', 39, 21.82728983461857], ['consumir', 297, 20.501775667071342], ['escuela', 21, 18.786609172821045], ['rural', 12, 15.42649231851101], ['hijo', 79, 15.404953621327877]]}, {'name': 'Tema 9', 'infoCompleta': [{'word': 'delincuencia', 'Importancia': 208.59701931476593, 'Frecuencia': 288}, {'word': 'población', 'Importancia': 39.45641964673996, 'Frecuencia': 1}, {'word': 'reciclaje', 'Importancia': 28.9220679551363, 'Frecuencia': 25}, {'word': 'visual', 'Importancia': 26.02279745042324, 'Frecuencia': 13}, {'word': 'laboral', 'Importancia': 21.602850407361984, 'Frecuencia': 76}, {'word': 'respetar', 'Importancia': 20.516205579042435, 'Frecuencia': 96}, {'word': 'temor', 'Importancia': 16.313737258315086, 'Frecuencia': 7}, {'word': 'cobro', 'Importancia': 14.163651503622532, 'Frecuencia': 2}, {'word': 'matanza', 'Importancia': 13.116583228111267, 'Frecuencia': 4}, {'word': 'compromiso', 'Importancia': 11.13847829401493, 'Frecuencia': 16}], 'words': [{'text': 'delincuencia', 'value': 1}, {'text': 'población', 'value': 1}, {'text': 'reciclaje', 'value': 1}, {'text': 'visual', 'value': 1}, {'text': 'laboral', 'value': 1}, {'text': 'respetar', 'value': 1}, {'text': 'temor', 'value': 1}, {'text': 'cobro', 'value': 1}, {'text': 'matanza', 'value': 1}, {'text': 'compromiso', 'value': 1}], 'ods': 1, 'odsComplementario': 16, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['delincuencia', 288, 208.59701931476593], ['población', 1, 39.45641964673996], ['reciclaje', 25, 28.9220679551363], ['visual', 13, 26.02279745042324], ['laboral', 76, 21.602850407361984], ['respetar', 96, 20.516205579042435], ['temor', 7, 16.313737258315086], ['cobro', 2, 14.163651503622532], ['matanza', 4, 13.116583228111267], ['compromiso', 16, 11.13847829401493]]}, {'name': 'Tema 10', 'infoCompleta': [{'word': 'familia', 'Importancia': 40.70432484149933, 'Frecuencia': 166}, {'word': 'peligro', 'Importancia': 39.74589332938194, 'Frecuencia': 3}, {'word': 'inseguro', 'Importancia': 33.97256135940552, 'Frecuencia': 28}, {'word': 'transitar', 'Importancia': 26.149917393922806, 'Frecuencia': 73}, {'word': 'cuerpo', 'Importancia': 17.02430471777916, 'Frecuencia': 3}, {'word': 'celular', 'Importancia': 16.108518466353416, 'Frecuencia': 36}, {'word': 'moto', 'Importancia': 16.01584255695343, 'Frecuencia': 70}, {'word': 'negocio', 'Importancia': 15.263151377439499, 'Frecuencia': 23}, {'word': 'construcción', 'Importancia': 13.294762931764126, 'Frecuencia': 6}, {'word': 'encontrar', 'Importancia': 13.272926211357117, 'Frecuencia': 43}], 'words': [{'text': 'familia', 'value': 1}, {'text': 'peligro', 'value': 1}, {'text': 'inseguro', 'value': 1}, {'text': 'transitar', 'value': 1}, {'text': 'cuerpo', 'value': 1}, {'text': 'celular', 'value': 1}, {'text': 'moto', 'value': 1}, {'text': 'negocio', 'value': 1}, {'text': 'construcción', 'value': 1}, {'text': 'encontrar', 'value': 1}], 'ods': 11, 'odsComplementario': 3, 'histogram': [['Palabra', 'Frecuencia', 'Importancia'], ['familia', 166, 40.70432484149933], ['peligro', 3, 39.74589332938194], ['inseguro', 28, 33.97256135940552], ['transitar', 73, 26.149917393922806], ['cuerpo', 3, 17.02430471777916], ['celular', 36, 16.108518466353416], ['moto', 70, 16.01584255695343], ['negocio', 23, 15.263151377439499], ['construcción', 6, 13.294762931764126], ['encontrar', 43, 13.272926211357117]]}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    numberOfTopics = 10\n",
    "    dotenv.load_dotenv(\".env\")\n",
    "    df = getData()\n",
    "    df = preprocess(df)\n",
    "    data_ready, stop_words = process(df)\n",
    "    lda_model, corpus, id2word = LDAModel(df,numberOfTopics,data_ready)\n",
    "    \n",
    "    topicsDictWords = createDictForTopics(lda_model, numberOfTopics)\n",
    "    odsDictWords = createDictForOds(df)\n",
    "    simmilarity = simmilarityOdsandTopics(topicsDictWords, odsDictWords)\n",
    "    frequency = frequentODS(simmilarity)\n",
    "    \n",
    "    storeGeneralInsight(df, frequency)\n",
    "    sankeyFile(lda_model,numberOfTopics)\n",
    "    topicData(lda_model,numberOfTopics,simmilarity, data_ready)\n",
    "    getChord(frequency,simmilarity)\n",
    "    swarnData(lda_model,numberOfTopics)\n",
    "    topicODSWeight(simmilarity)\n",
    "    dominantData = dominant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        print(new_doc[0])\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-532-a2642ed2f5e3>:55: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['erradicar', 'pobreza', 'extremo'], ['delincuencia']]\n",
      "erradicar\n",
      "pobreza\n",
      "extremo\n",
      "0.130*\"delincuencia\" + 0.074*\"pobreza\" + 0.067*\"robo\" + 0.054*\"intolerancia\" + 0.023*\"autoridad\" + 0.021*\"derecho\" + 0.019*\"hambre\" + 0.018*\"comida\" + 0.015*\"carrera\" + 0.013*\"favor\"\n",
      "[(5, 0.361618), (0, 0.105452776), (7, 0.10234078), (3, 0.09531839), (2, 0.07395846), (9, 0.07305649), (8, 0.07300385), (4, 0.047406573), (1, 0.040699434), (6, 0.027145283)]\n",
      "delincuencia\n",
      "0.130*\"delincuencia\" + 0.074*\"pobreza\" + 0.067*\"robo\" + 0.054*\"intolerancia\" + 0.023*\"autoridad\" + 0.021*\"derecho\" + 0.019*\"hambre\" + 0.018*\"comida\" + 0.015*\"carrera\" + 0.013*\"favor\"\n",
      "[(5, 0.23636119), (0, 0.12602156), (7, 0.12231233), (3, 0.11394797), (2, 0.08840623), (9, 0.087373696), (8, 0.08735835), (4, 0.056807492), (1, 0.04881039), (6, 0.0326008)]\n"
     ]
    }
   ],
   "source": [
    "encontrarODSTopico(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insightsDominant(dominantData):\n",
    "    dominantData.groupby(['Dominant_Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilalonart/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>inseguridad, vicio, venta, biblioteca, robar, ...</td>\n",
       "      <td>[rios, hombre, rios]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3116</td>\n",
       "      <td>empleo, oportunidad, trabajo, universidad, col...</td>\n",
       "      <td>[drogar, joven, negocio, mujer, joven]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3841</td>\n",
       "      <td>inseguridad, vicio, venta, biblioteca, robar, ...</td>\n",
       "      <td>[empezar, jovenes, viciar, antejardines, mujer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3398</td>\n",
       "      <td>plata, dinero, peligro, inseguro, transitar, m...</td>\n",
       "      <td>[vender, pecho, pobreza, creer, negociar, crec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5560</td>\n",
       "      <td>aire, libre, convivencia, pueblo, orden, prueb...</td>\n",
       "      <td>[aquejar, convivencia, santa_elena, mezclar, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>7676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3076</td>\n",
       "      <td>inseguridad, vicio, venta, biblioteca, robar, ...</td>\n",
       "      <td>[aquejar, inseguridad, robo, moto, bicicleta, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>7677</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4445</td>\n",
       "      <td>empleo, oportunidad, trabajo, universidad, col...</td>\n",
       "      <td>[migrante, cavar, desempleo, hombre, emplear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>7678</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>empleo, oportunidad, trabajo, universidad, col...</td>\n",
       "      <td>[desempleo, hombre, emplear, jul]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>7679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3816</td>\n",
       "      <td>inseguridad, vicio, venta, biblioteca, robar, ...</td>\n",
       "      <td>[recoger, respetar, escenario, biblioteca, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>7680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5015</td>\n",
       "      <td>inseguridad, vicio, venta, biblioteca, robar, ...</td>\n",
       "      <td>[aquejar, inseguridad, intranquilidad, bullir,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7681 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0               0             0.0              0.1857   \n",
       "1               1             7.0              0.3116   \n",
       "2               2             0.0              0.3841   \n",
       "3               3             2.0              0.3398   \n",
       "4               4             8.0              0.5560   \n",
       "...           ...             ...                 ...   \n",
       "7676         7676             0.0              0.3076   \n",
       "7677         7677             7.0              0.4445   \n",
       "7678         7678             7.0              0.4451   \n",
       "7679         7679             0.0              0.3816   \n",
       "7680         7680             0.0              0.5015   \n",
       "\n",
       "                                               Keywords  \\\n",
       "0     inseguridad, vicio, venta, biblioteca, robar, ...   \n",
       "1     empleo, oportunidad, trabajo, universidad, col...   \n",
       "2     inseguridad, vicio, venta, biblioteca, robar, ...   \n",
       "3     plata, dinero, peligro, inseguro, transitar, m...   \n",
       "4     aire, libre, convivencia, pueblo, orden, prueb...   \n",
       "...                                                 ...   \n",
       "7676  inseguridad, vicio, venta, biblioteca, robar, ...   \n",
       "7677  empleo, oportunidad, trabajo, universidad, col...   \n",
       "7678  empleo, oportunidad, trabajo, universidad, col...   \n",
       "7679  inseguridad, vicio, venta, biblioteca, robar, ...   \n",
       "7680  inseguridad, vicio, venta, biblioteca, robar, ...   \n",
       "\n",
       "                                                   Text  \n",
       "0                                  [rios, hombre, rios]  \n",
       "1                [drogar, joven, negocio, mujer, joven]  \n",
       "2     [empezar, jovenes, viciar, antejardines, mujer...  \n",
       "3     [vender, pecho, pobreza, creer, negociar, crec...  \n",
       "4     [aquejar, convivencia, santa_elena, mezclar, n...  \n",
       "...                                                 ...  \n",
       "7676  [aquejar, inseguridad, robo, moto, bicicleta, ...  \n",
       "7677  [migrante, cavar, desempleo, hombre, emplear, ...  \n",
       "7678                  [desempleo, hombre, emplear, jul]  \n",
       "7679  [recoger, respetar, escenario, biblioteca, con...  \n",
       "7680  [aquejar, inseguridad, intranquilidad, bullir,...  \n",
       "\n",
       "[7681 rows x 5 columns]"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominantData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilalonart/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def dominant():\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        print(contents)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    doc_lens = [[' ', len(d)] for d in df_dominant_topic.Text]\n",
    "    doc_lens.insert(0,['ODS','Length'])\n",
    "    json_result = json.dumps(doc_lens, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/histogram.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilalonart/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                            [rios, rios]\n",
      "1                  [drogar, joven, negocio, mujer, joven]\n",
      "2       [empezar, jovenes, viciar, antejardines, mujer...\n",
      "3       [vender, pecho, pobreza, creer, negociar, crec...\n",
      "4       [aquejar, convivencia, santa_elena, mezclar, n...\n",
      "                              ...                        \n",
      "7676    [aquejar, inseguridad, robo, moto, bicicleta, ...\n",
      "7677           [migrante, cavar, desempleo, emplear, jul]\n",
      "7678                            [desempleo, emplear, jul]\n",
      "7679    [recoger, respetar, escenario, biblioteca, con...\n",
      "7680    [aquejar, inseguridad, intranquilidad, bullir,...\n",
      "Length: 7681, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>contaminación, ambiental, parque, aire, pobrez...</td>\n",
       "      <td>[rios, rios]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>contaminación, ambiental, parque, aire, pobrez...</td>\n",
       "      <td>[drogar, joven, negocio, mujer, joven]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3428</td>\n",
       "      <td>violencia, vicio, ley, expendio, estudiar, int...</td>\n",
       "      <td>[empezar, jovenes, viciar, antejardines, mujer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5114</td>\n",
       "      <td>familia, peligro, inseguro, transitar, cuerpo,...</td>\n",
       "      <td>[vender, pecho, pobreza, creer, negociar, crec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>contaminación, ambiental, parque, aire, pobrez...</td>\n",
       "      <td>[aquejar, convivencia, santa_elena, mezclar, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>7676</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>contaminación, ambiental, parque, aire, pobrez...</td>\n",
       "      <td>[aquejar, inseguridad, robo, moto, bicicleta, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>7677</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4520</td>\n",
       "      <td>empleo, oportunidad, trabajo, biblioteca, trab...</td>\n",
       "      <td>[migrante, cavar, desempleo, emplear, jul]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>7678</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>empleo, oportunidad, trabajo, biblioteca, trab...</td>\n",
       "      <td>[desempleo, emplear, jul]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>7679</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3181</td>\n",
       "      <td>empleo, oportunidad, trabajo, biblioteca, trab...</td>\n",
       "      <td>[recoger, respetar, escenario, biblioteca, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>7680</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>inseguridad, venta, desigualdad, robo, sexual,...</td>\n",
       "      <td>[aquejar, inseguridad, intranquilidad, bullir,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7681 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0               0             2.0              0.1782   \n",
       "1               1             2.0              0.3800   \n",
       "2               2             5.0              0.3428   \n",
       "3               3             9.0              0.5114   \n",
       "4               4             2.0              0.5013   \n",
       "...           ...             ...                 ...   \n",
       "7676         7676             2.0              0.3051   \n",
       "7677         7677             7.0              0.4520   \n",
       "7678         7678             7.0              0.4525   \n",
       "7679         7679             7.0              0.3181   \n",
       "7680         7680             3.0              0.4843   \n",
       "\n",
       "                                               Keywords  \\\n",
       "0     contaminación, ambiental, parque, aire, pobrez...   \n",
       "1     contaminación, ambiental, parque, aire, pobrez...   \n",
       "2     violencia, vicio, ley, expendio, estudiar, int...   \n",
       "3     familia, peligro, inseguro, transitar, cuerpo,...   \n",
       "4     contaminación, ambiental, parque, aire, pobrez...   \n",
       "...                                                 ...   \n",
       "7676  contaminación, ambiental, parque, aire, pobrez...   \n",
       "7677  empleo, oportunidad, trabajo, biblioteca, trab...   \n",
       "7678  empleo, oportunidad, trabajo, biblioteca, trab...   \n",
       "7679  empleo, oportunidad, trabajo, biblioteca, trab...   \n",
       "7680  inseguridad, venta, desigualdad, robo, sexual,...   \n",
       "\n",
       "                                                   Text  \n",
       "0                                          [rios, rios]  \n",
       "1                [drogar, joven, negocio, mujer, joven]  \n",
       "2     [empezar, jovenes, viciar, antejardines, mujer...  \n",
       "3     [vender, pecho, pobreza, creer, negociar, crec...  \n",
       "4     [aquejar, convivencia, santa_elena, mezclar, n...  \n",
       "...                                                 ...  \n",
       "7676  [aquejar, inseguridad, robo, moto, bicicleta, ...  \n",
       "7677         [migrante, cavar, desempleo, emplear, jul]  \n",
       "7678                          [desempleo, emplear, jul]  \n",
       "7679  [recoger, respetar, escenario, biblioteca, con...  \n",
       "7680  [aquejar, inseguridad, intranquilidad, bullir,...  \n",
       "\n",
       "[7681 rows x 5 columns]"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
