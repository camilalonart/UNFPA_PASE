{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import argparse\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "import spacy\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getData():\n",
    "    df = pd.read_json(r'../server/data/MedellinCleaned.json')\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def preprocess(df):\n",
    "    #Remover filas duplicadas\n",
    "    df = df.drop_duplicates(subset=['respuesta'])\n",
    "\n",
    "    #remover espacios extra entre palabras\n",
    "    df['respuesta'] = df['respuesta'].str.replace('  ',' ')\n",
    "    df['respuesta'] = df['respuesta'].str.strip()\n",
    "    #transformar a minuscula\n",
    "    df['respuesta'] = df['respuesta'].str.lower()\n",
    "    #eliminar puntuacion\n",
    "    df['respuesta'] = df['respuesta'].str.replace(r'\\s+', ' ')\n",
    "    df['respuesta'] = df['respuesta'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "    #ods de ods_n como string a n entero\n",
    "    df.ods = [ int(data[data.find('_')+1:])for data in df.ods ]\n",
    "    #meta de meta_ods_n como string a n entero\n",
    "    df.meta = [ data[data.rfind('_')+1:] for data in df.meta ]\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def process(df):\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    stop_words.extend(['importante','saber','caso','decirlo''esperar','servicio','colombia','decirlo','completamente','encontrar','negocio','respetar','rural','inclusive','loma','libre','hablar','andar','hijo','cuerpo','visual','compromiso','tampoco','seguir','grave','situación','manera','mejorar','poner','hecho','villanueva','jul','may','aug','nuevo','desarrollar','totalmente','aquejar','general','casar','necesitar','pensar','sentir','comunicar','conseguir','disfrutar','darle','mano','alto','claro','bello','buscar','viendo','zonas','tal','podría','afectando','primer','aún','mismos','sólo','digo','aunque','mal','encuentra','cuanto','diferentes','hoy','diría','allá','dejan','arribar','dejar','regar','suceder','vacuno','ayudar','gustar','taco','ninos','jun','ocasionar','evidenciar','pedir','apartar','demorar','aguar','fuerte','inseguro','horas','habitantes','persona','poder','pronto','lugar','empiezan','alguien','presentando','actualmente','saben', 'cosa', 'sido','pues','así','acá','siempre','tan','sector','decir','pasar','mundo','territorio','siendo','varios','verdad','principalmente', 'partes','causa','hacia','sabe','sido','quiere','quién','saben','cosa','sido','tantos', 'salir', 'nunca','calles','visto','mayor','gran','dónde','veo','poca','dice','cada','da','ir','nadie','enfrenta','podemos','casi','menos','cuenta','haciendo','hacen','tipo','bien','digamos','primero','haciendo','cantidad','forma','lado','dar','después','últimamente','mejor','debido','basura','pico','carro','horrible','realmente','toda','consideró','quedan','siempre','día','problemáticas','cualquier','problemática','día','hora','genera','nivel','falta','principales','presenta','pueden','ejemplo','grande','mala','pasan','ahora','van','considero','manejan','todas','dos','parece','personas','tener','pasan','parte','creo','cuantas','segundo','veces','muchas','tema','personar','medellín','días','ciudad','barrio','gente','problemas','tiene','sector','ser','llegar','presentar','bueno','falto','generar','pues','así','acá','hace','ver','vez','si','generla','cierto','piso','mientras','ahí','cómo','pasando','capacidad','ninguna','tantas','toca','sectores','ven','recogen','va','debería','buenos','sabemos','ciertas','sé','necesita','tan','aquí','sino','años','pertenencia','caminar','prado','deporte','mes','mendicidad','atender','altavista','pie','dificultad','incluso','ve','cosas','puede','afecta','daniel','solamente','edad','barrios','atención','vivir','tolerancia','frente','municipio','comunidad','común','cuidado','vivo','buen','grandes','partir','social','difícil','vida','prueba','dominantData','sacarlo','orden','pueblo','sol','hombre','actual','imposible','intolerancia','tarde','dicho','ahorita','pasa','obviamente','robledo','afectar','pesar','semana','bajar','sale','ninguna','simple','altamente','diferente','margen','comunicación','temas','empresa','derecho','hermano','familiar','constantemente','demasiados','cultura','seguro','mantenimiento','debe','considera','aspectos','poquito','venir','punto','peor','responsabilidad','factor','entorno','llevar','medio','uso','tranquilo','sitio','favor','todavía','cerca','mañana','momentos','apoyo','lleva','considera','mayoría','alrededor','vereda','fácil','presentan','sacan','llegan','necesitamos','grupos','necesidad','puesto','vecino','país','camino','público','vamos','usted','dentro','ponen','segunda','zona','comunas','deben','sociedad','san_cristóbal','san_antonio','acompañamiento','tiempo','bastante','comuna','definitivamente','comuna','dando','buena','buenas','dan','secundario','afectan','mantienen','centro','existe','año','recursos','sacar','calidad','necesidades','corregimiento','malo','vemos','pase','san_cristóbal','belén','lugares','lugar','espacio','espacios','segundo','manejo','queda','tanta','demasiado','tambien','mas','segundar','problema','demás','igual','casas','problemática','entonces','hacer','mucho','quedo','mismo','momento','pienso','principal','mucha'])\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "    df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sent in sentences:\n",
    "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "            yield(sent)  \n",
    "\n",
    "    # Convert to list\n",
    "    data = df.values.tolist()\n",
    "    data_words = list(sent_to_words(data))\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # !python3 -m spacy download en  # run in terminal once\n",
    "    def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "        texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "        texts = [bigram_mod[doc] for doc in texts]\n",
    "        texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "        texts_out = []\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        # remove stopwords once more after lemmatization\n",
    "        texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "        return texts_out\n",
    "\n",
    "    data_ready = process_words(data_words)  # processed Text Data!\n",
    "    return data_ready, stop_words\n",
    "\n",
    "def LDAModel(df,numberOfTopics,data_ready):\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    texts = data_ready\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=numberOfTopics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True)\n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def modelPerplexityCoherenceScore(lda_model,data_words_trigrams):\n",
    "    # Compute Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return perplexity, coherence_lda\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeTopicsData(model, corpus, id2word, stop, numberOfTopics):\n",
    "    model_info = []\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    for topic, words in model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        words_dict = {}\n",
    "        for word, importance in words:\n",
    "            word_obj = {'text': word, 'value': float(importance) * 100}\n",
    "            words_dict[word] = word_obj\n",
    "        topic = {\n",
    "          'words': list(words_dict.values()),\n",
    "          'color': colors[topic],\n",
    "        }\n",
    "        model_info.append(topic)\n",
    "    json_result = json.dumps(model_info, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/result.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return model_info\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeGeneralInsight(df, odsData):\n",
    "    jsonData = {}\n",
    "    #ANIO--------------------------------------------------\n",
    "    jsonData['anio'] = [int(df.anio[0])]\n",
    "    \n",
    "    #RESPUESTAS POR PREGUNTA-----------------------------------------------\n",
    "    preguntasCount = df['pregunta'].value_counts()\n",
    "    cantidadDePreguntas = len(preguntasCount)\n",
    "    preguntasDict = {}\n",
    "    for preguntaP, cantidadP in preguntasCount.iteritems():\n",
    "        preguntasDict[preguntaP] = {'pregunta': preguntaP, 'cantidad': cantidadP}\n",
    "    preguntasListas = list(preguntasDict.values())\n",
    "    jsonData['preguntas'] = list(preguntasDict.values())\n",
    "    \n",
    "    #NUMERO DE RESPUESTAS--------------------------------------------------\n",
    "    total = 0\n",
    "    for p in preguntasListas:\n",
    "        total = max(total, p['cantidad'])\n",
    "    jsonData['totalRespuestas'] = total\n",
    "    \n",
    "    #RESPUESTAS POR EDAD-----------------------------------------------\n",
    "    edades = []\n",
    "    for edad, cantidad in df['rangoEdad'].value_counts().iteritems():\n",
    "        edades.append([edad,cantidad//cantidadDePreguntas])\n",
    "    edades.sort(key = lambda x: x[0])\n",
    "    jsonData['edad'] = edades\n",
    "    \n",
    "    #RESPUESTAS POR SEXO-----------------------------------------------\n",
    "    sexos = []\n",
    "    for genero, cantidad in df['sexo'].value_counts().iteritems():\n",
    "        sexoActual = {\"sexoNombre\": genero, \"value\": cantidad//cantidadDePreguntas}\n",
    "        sexos.append(sexoActual)\n",
    "    jsonData['sexo'] = sexos\n",
    "    \n",
    "    #RESPUESTAS POR ODS------------------------------------------------\n",
    "    \n",
    "    jsonData['porOds'] = odsData\n",
    "    #TOP DE PALABRAS----------------------------------------------------\n",
    "    topPalabras = []\n",
    "    for numero, cantidad in df['palabra'].value_counts().iteritems():\n",
    "        topPalabras.append([numero, cantidad])\n",
    "    jsonData['topPalabras'] = topPalabras[:8]\n",
    "    #CANTIDAD DE ODS POR MES--------------------------------------------\n",
    "    ''' ODS en x y una linea por mes donde y es la cantidad de ods\n",
    "        [['../images/SDGs/1.png',\n",
    "          1,\n",
    "          [1, 0],\n",
    "          [2, 0],\n",
    "          [3, 0],\n",
    "          [4, 18],\n",
    "          [5, 496],\n",
    "          [6, 234],\n",
    "          [7, 192],\n",
    "          [8, 22],\n",
    "          [9, 0],\n",
    "          [10, 0],\n",
    "          [11, 0],\n",
    "          [12, 0]],\n",
    "    '''\n",
    "    datosODS = {}\n",
    "    datosPorMes = df.groupby(\"mesTexo\")\n",
    "    for mes, datos in datosPorMes:\n",
    "        for ods, cantidad in datos['ods'].value_counts().iteritems():\n",
    "            if ods not in datosODS:\n",
    "                datosODS[ods] = {'ods':ods}\n",
    "            datosODS[ods][mes] = cantidad\n",
    "    for ods in datosODS:\n",
    "        for mes, _ in datosPorMes:\n",
    "            if mes not in datosODS[ods]:\n",
    "                datosODS[ods][mes] = 0\n",
    "\n",
    "    baseImage = '../images/SDGs/'\n",
    "    mesesTable = {\"Ene\":1,\"Feb\":2,\"Mar\":3,\"Abr\":4,\"May\":5,\"Jun\":6,\"Jul\":7,\"Aug\":8,\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dic\":12}\n",
    "    datosODS = list(datosODS.values())\n",
    "    datosODS.sort(key = lambda x: x['ods'])\n",
    "    datosEnLista = []\n",
    "    for entrada in datosODS:\n",
    "        listVersion = []\n",
    "        for key, val in entrada.items():\n",
    "            listVersion.append([key,val])\n",
    "        monthsAlreadyIn = set()\n",
    "        for i in range(1,len(listVersion)):\n",
    "            monthsAlreadyIn.add(mesesTable[listVersion[i][0]])\n",
    "            listVersion[i][0] = mesesTable[listVersion[i][0]]\n",
    "        for i in range(1,13):\n",
    "            if i not in monthsAlreadyIn:\n",
    "                listVersion.append([i,0])\n",
    "        odsS = listVersion[1:]   \n",
    "        odsS.sort(key = lambda x: x[0])\n",
    "        #listVersion[0][0] = baseImage + str(listVersion[0][1]) + '.png'\n",
    "        #listVersion = listVersion[0] + odsS\n",
    "        datosEnLista.append(listVersion)\n",
    "    jsonData['datosPorMes'] = datosEnLista\n",
    "\n",
    "    json_result = json.dumps(jsonData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/generalResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return jsonData  \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def sankeyFile(lda_model,numberOfTopics):\n",
    "    topicsData = lda_model.show_topics(num_topics = numberOfTopics,num_words=7,formatted=False)\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    for topic in topicsData:\n",
    "        currTopic = -1\n",
    "        for words in topic:\n",
    "            if type(words) == int:\n",
    "                currTopic = words\n",
    "            else:\n",
    "                for word, weight in words:\n",
    "                    sankeyRow = ['Tema '+str(currTopic+1),word,int(weight*100)]\n",
    "                    dataForSankey.append(sankeyRow)\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/sankeyResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------       \n",
    "def createDictForTopics(lda_model, numberOfTopics):\n",
    "    topicsDictWords = {}\n",
    "    i=1\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        for word, value in words:\n",
    "            listOfTopic.append(word)\n",
    "        topicsDictWords[i] = listOfTopic   \n",
    "        i+=1\n",
    "    return topicsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def createDictForOds(df):\n",
    "    odsDictWords = {}\n",
    "    lista8 = ['desempleo','empleo','oportunidades','trabajo','desempleado','desempleados''oportunidad','oportunidades','plata','salario','trabajador','trabajo','aquejar','esquinar','trabajar','laboral']\n",
    "    for row_id, data in df.iterrows():\n",
    "        if data['palabra'] in lista8 and data['ods'] != 8:\n",
    "            continue\n",
    "        if data['ods'] not in odsDictWords:        \n",
    "            odsDictWords[data['ods']] = set()\n",
    "        if len(data['palabra'])>=3:\n",
    "            odsDictWords[data['ods']].add(data['palabra'])    \n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def simmilarityOdsandTopics(topicsDictWords, odsDictWords):\n",
    "    simmilarityTopics = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        lista = []\n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            num = len(set(list1) & set(list2))\n",
    "            lista.append({'ods': odsNum,'Similaridad': num})\n",
    "        datosTopico = {'topico':'Tema'+str(topicNum), 'SimilaridadOds' : lista}\n",
    "        simmilarityTopics.append(datosTopico)\n",
    "    return simmilarityTopics\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def frequentODS(simmilarity):\n",
    "    odsTopOrder = {}\n",
    "    for topic in simmilarity:\n",
    "        for data in topic['SimilaridadOds']:\n",
    "            if data['ods'] in odsTopOrder:\n",
    "                odsTopOrder[data['ods']] += int(data['Similaridad'])\n",
    "            else:\n",
    "                odsTopOrder[data['ods']] = int(data['Similaridad'])\n",
    "    odsTopOrder = list(odsTopOrder.items())\n",
    "    odsTopOrder.sort(key = lambda x: x[1], reverse = True)\n",
    "    #change format\n",
    "    ods = []\n",
    "    for numero, cantidad in odsTopOrder:\n",
    "        ods.append([numero, cantidad])\n",
    "    return ods\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights):\n",
    "    data_flat = [w for w_list in data_ready for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "    moreDataPointer = 0\n",
    "    i=1\n",
    "    json_data = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        histogram = [['Palabra','Frecuencia','Importancia']]\n",
    "        infoCompleta = []\n",
    "        for word, value in words:\n",
    "            word_count = counter[word]\n",
    "            listOfTopic.append({\"text\": word,\"value\":1})\n",
    "            infoCompleta.append({\"word\":word, 'Importancia':float(value*1000), 'Frecuencia':word_count})\n",
    "            histogram.append([word,word_count, float(value*1000)])\n",
    "        lista = []\n",
    "        for t in simmilarity:\n",
    "            if t['topico']=='Tema'+str(topic+1):\n",
    "                for data in t['SimilaridadOds']:\n",
    "                    lista.append([data['ods'],data['Similaridad']])\n",
    "        lista.sort(key = lambda x: x[1], reverse = True)\n",
    "        odsRelacionado = lista[0][0]\n",
    "        odsComplementario = lista[1][0]\n",
    "        json_data.append({\"name\":\"Tema \"+str(topic+1),'infoCompleta':infoCompleta,\"words\":listOfTopic,\"ods\":odsRelacionado, \"sexo\":moreInsights[moreDataPointer]['sexo'], 'edades': moreInsights[moreDataPointer]['edades'], \"odsComplementario\": odsComplementario,'histogram':histogram}) \n",
    "        moreDataPointer+=1\n",
    "        i+=1\n",
    "    json_result = json.dumps(json_data, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/dataPerTopic.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getChord(frequency,simmilarity):\n",
    "    frequency=frequency[:6]\n",
    "    names = []\n",
    "    dataForODS = {}\n",
    "    for ods, quatity in frequency:\n",
    "        names.append(ods)\n",
    "    for data in simmilarity:\n",
    "        for row in data['SimilaridadOds']:\n",
    "            if row['ods'] in names and row['Similaridad'] > 1:\n",
    "                if row['ods'] in dataForODS:\n",
    "                    dataForODS[row['ods']].append(data['topico'])\n",
    "                else:\n",
    "                    dataForODS[row['ods']] = [data['topico']]\n",
    "    chordData = []\n",
    "    nameData = []\n",
    "    for names, temas in dataForODS.items():\n",
    "        nameData.append('ODS'+str(names))\n",
    "        array = [int(temaname[temaname.find('a')+1:]) for temaname in temas]\n",
    "        if len(array) > 8: array = array[:8]\n",
    "        if len(array) < 8: array = array + ([0]*(8-len(array)))\n",
    "        chordData.append(array)\n",
    "    json_result = json.dumps(chordData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(nameData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def swarnData(lda_model,numberOfTopics):\n",
    "    topicsDictWords = []\n",
    "    i=1\n",
    "    swarnNames = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        swarnNames.append('Tema'+str(topic+1))\n",
    "        for word, value in words:\n",
    "            topicsDictWords.append({\"group\":'Tema'+str(topic+1),\"id\":word,\"value\":int(value*100),\"volume\":int(value*100)})\n",
    "        i+=1\n",
    "    json_result = json.dumps(swarnNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(topicsDictWords, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicODSWeight(simmilarity):\n",
    "    dataBars = []\n",
    "    keysNames = set()\n",
    "    for data in simmilarity:\n",
    "        odsResults = {'Temas': data['topico']}\n",
    "        for odsDetails in data['SimilaridadOds']:\n",
    "            keysNames.add(str(odsDetails['ods']))\n",
    "            odsResults[str(odsDetails['ods'])] = int(odsDetails['Similaridad'])\n",
    "        dataBars.append(odsResults)\n",
    "    keysNames = list(keysNames)\n",
    "    json_result = json.dumps(dataBars, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentage.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(keysNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentageKeys.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def dominant():\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sexoSerie = df['sexo']\n",
    "        rangoEdadSerie = df['rangoEdad']\n",
    "        sent_topics_df = pd.concat([sent_topics_df, sexoSerie, rangoEdadSerie, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    \n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Sexo','rangoEdad','Text']\n",
    "    doc_lens = [['ODS','Length']]\n",
    "    for d in df_dominant_topic.Text:\n",
    "        if type(d)==list:\n",
    "            doc_lens.append([' ', len(d)])\n",
    "        else:\n",
    "            doc_lens.append([' ', 0])\n",
    "    json_result = json.dumps(doc_lens, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/histogram.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return df_dominant_topic\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def moreTopicInsights(dominantData):\n",
    "    groupsByTopic = dominantData.groupby(['Dominant_Topic'])\n",
    "    topicData = []\n",
    "    i = 0\n",
    "    for row in groupsByTopic:\n",
    "        topic = 'Tema '+ str(i+1)\n",
    "        countsSexo = row[1]['Sexo'].value_counts()\n",
    "        countsRangoEdad = row[1]['rangoEdad'].value_counts()\n",
    "        countsEdad = []\n",
    "        for edad, cantidad in countsRangoEdad.iteritems():\n",
    "            countsEdad.append([edad,cantidad])\n",
    "        countsEdad.sort(key = lambda x: x[0])\n",
    "        sexos = []\n",
    "        for genero, cantidad in countsSexo.iteritems():\n",
    "            sexos.append({\"sexoNombre\": genero, \"value\": cantidad})\n",
    "        topicData.append({'topic':topic, 'sexo':sexos, 'edades': countsEdad})\n",
    "        i+=1\n",
    "    return topicData\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        print(new_doc[0])\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-1ff5ccba9fa9>:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(r'\\s+', ' ')\n",
      "<ipython-input-16-1ff5ccba9fa9>:40: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace('[{}]'.format(string.punctuation), '')\n",
      "<ipython-input-16-1ff5ccba9fa9>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    numberOfTopics = 10\n",
    "    dotenv.load_dotenv(\".env\")\n",
    "    df = getData()\n",
    "    df = preprocess(df)\n",
    "    data_ready, stop_words = process(df)\n",
    "    lda_model, corpus, id2word = LDAModel(df,numberOfTopics,data_ready)\n",
    "    \n",
    "    topicsDictWords = createDictForTopics(lda_model, numberOfTopics)\n",
    "    odsDictWords = createDictForOds(df)\n",
    "    simmilarity = simmilarityOdsandTopics(topicsDictWords, odsDictWords)\n",
    "    frequency = frequentODS(simmilarity)\n",
    "    \n",
    "    dominantData = dominant()\n",
    "    moreInsights = moreTopicInsights(dominantData)\n",
    "    storeGeneralInsight(df, frequency)\n",
    "    sankeyFile(lda_model,numberOfTopics)\n",
    "    topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights)\n",
    "    getChord(frequency,simmilarity)\n",
    "    swarnData(lda_model,numberOfTopics)\n",
    "    topicODSWeight(simmilarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        print(new_doc[0])\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-532-a2642ed2f5e3>:55: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['respuesta'] = df['respuesta'].str.replace(pattern, '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['erradicar', 'pobreza', 'extremo'], ['delincuencia']]\n",
      "erradicar\n",
      "pobreza\n",
      "extremo\n",
      "0.130*\"delincuencia\" + 0.074*\"pobreza\" + 0.067*\"robo\" + 0.054*\"intolerancia\" + 0.023*\"autoridad\" + 0.021*\"derecho\" + 0.019*\"hambre\" + 0.018*\"comida\" + 0.015*\"carrera\" + 0.013*\"favor\"\n",
      "[(5, 0.361618), (0, 0.105452776), (7, 0.10234078), (3, 0.09531839), (2, 0.07395846), (9, 0.07305649), (8, 0.07300385), (4, 0.047406573), (1, 0.040699434), (6, 0.027145283)]\n",
      "delincuencia\n",
      "0.130*\"delincuencia\" + 0.074*\"pobreza\" + 0.067*\"robo\" + 0.054*\"intolerancia\" + 0.023*\"autoridad\" + 0.021*\"derecho\" + 0.019*\"hambre\" + 0.018*\"comida\" + 0.015*\"carrera\" + 0.013*\"favor\"\n",
      "[(5, 0.23636119), (0, 0.12602156), (7, 0.12231233), (3, 0.11394797), (2, 0.08840623), (9, 0.087373696), (8, 0.08735835), (4, 0.056807492), (1, 0.04881039), (6, 0.0326008)]\n"
     ]
    }
   ],
   "source": [
    "encontrarODSTopico(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
