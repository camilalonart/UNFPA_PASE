{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import argparse\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "import csv\n",
    "\n",
    "import spacy\n",
    "\n",
    "def LDAModel(df,numberOfTopics,data_ready):\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=numberOfTopics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=1500,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True)\n",
    "\n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def modelPerplexityCoherenceScore(lda_model,data_words_trigrams):\n",
    "    # Compute Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return perplexity, coherence_lda\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeGeneralInsight(df, odsData):\n",
    "    jsonData = {}\n",
    "    #ANIO--------------------------------------------------\n",
    "    jsonData['anio'] = [int(df.anio[0])]\n",
    "    \n",
    "    #RESPUESTAS POR PREGUNTA-----------------------------------------------\n",
    "    preguntasCount = df['pregunta'].value_counts()\n",
    "    cantidadDePreguntas = len(preguntasCount)\n",
    "    preguntasDict = {}\n",
    "    for preguntaP, cantidadP in preguntasCount.iteritems():\n",
    "        preguntasDict[preguntaP] = {'pregunta': preguntaP, 'cantidad': cantidadP}\n",
    "    preguntasListas = list(preguntasDict.values())\n",
    "    jsonData['preguntas'] = list(preguntasDict.values())\n",
    "    \n",
    "    #NUMERO DE RESPUESTAS--------------------------------------------------\n",
    "    total = 0\n",
    "    for p in preguntasListas:\n",
    "        total = max(total, p['cantidad'])\n",
    "    jsonData['totalRespuestas'] = total\n",
    "    \n",
    "    #RESPUESTAS POR EDAD-----------------------------------------------\n",
    "    edades = []\n",
    "    for edad, cantidad in df['rangoEdad'].value_counts().iteritems():\n",
    "        edades.append([edad,cantidad//cantidadDePreguntas])\n",
    "    edades.sort(key = lambda x: x[0])\n",
    "    jsonData['edad'] = edades\n",
    "    \n",
    "    #RESPUESTAS POR SEXO-----------------------------------------------\n",
    "    sexos = []\n",
    "    for genero, cantidad in df['sexo'].value_counts().iteritems():\n",
    "        sexoActual = {\"sexoNombre\": genero, \"value\": cantidad//cantidadDePreguntas}\n",
    "        sexos.append(sexoActual)\n",
    "    jsonData['sexo'] = sexos\n",
    "    \n",
    "    #RESPUESTAS POR ODS------------------------------------------------\n",
    "    \n",
    "    jsonData['porOds'] = odsData\n",
    "    #TOP DE PALABRAS----------------------------------------------------\n",
    "    topPalabras = []\n",
    "    for numero, cantidad in df['palabra'].value_counts().iteritems():\n",
    "        topPalabras.append([numero, cantidad])\n",
    "    jsonData['topPalabras'] = topPalabras[:8]\n",
    "    #CANTIDAD DE ODS POR MES--------------------------------------------\n",
    "    ''' ODS en x y una linea por mes donde y es la cantidad de ods\n",
    "        [['../images/SDGs/1.png',\n",
    "          1,\n",
    "          [1, 0],\n",
    "          [2, 0],\n",
    "          [3, 0],\n",
    "          [4, 18],\n",
    "          [5, 496],\n",
    "          [6, 234],\n",
    "          [7, 192],\n",
    "          [8, 22],\n",
    "          [9, 0],\n",
    "          [10, 0],\n",
    "          [11, 0],\n",
    "          [12, 0]],\n",
    "    '''\n",
    "    datosODS = {}\n",
    "    datosPorMes = df.groupby(\"mesTexo\")\n",
    "    for mes, datos in datosPorMes:\n",
    "        for ods, cantidad in datos['ods'].value_counts().iteritems():\n",
    "            if ods not in datosODS:\n",
    "                datosODS[ods] = {'ods':ods}\n",
    "            datosODS[ods][mes] = cantidad\n",
    "    for ods in datosODS:\n",
    "        for mes, _ in datosPorMes:\n",
    "            if mes not in datosODS[ods]:\n",
    "                datosODS[ods][mes] = 0\n",
    "\n",
    "    baseImage = '../images/SDGs/'\n",
    "    mesesTable = {\"Ene\":1,\"Feb\":2,\"Mar\":3,\"Abr\":4,\"May\":5,\"Jun\":6,\"Jul\":7,\"Aug\":8,\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dic\":12}\n",
    "    datosODS = list(datosODS.values())\n",
    "    datosODS.sort(key = lambda x: x['ods'])\n",
    "    datosEnLista = []\n",
    "    for entrada in datosODS:\n",
    "        listVersion = []\n",
    "        for key, val in entrada.items():\n",
    "            listVersion.append([key,val])\n",
    "        monthsAlreadyIn = set()\n",
    "        for i in range(1,len(listVersion)):\n",
    "            monthsAlreadyIn.add(mesesTable[listVersion[i][0]])\n",
    "            listVersion[i][0] = mesesTable[listVersion[i][0]]\n",
    "        for i in range(1,13):\n",
    "            if i not in monthsAlreadyIn:\n",
    "                listVersion.append([i,0])\n",
    "        odsS = listVersion[1:]   \n",
    "        odsS.sort(key = lambda x: x[0])\n",
    "        #listVersion[0][0] = baseImage + str(listVersion[0][1]) + '.png'\n",
    "        #listVersion = listVersion[0] + odsS\n",
    "        datosEnLista.append(listVersion)\n",
    "    jsonData['datosPorMes'] = datosEnLista\n",
    "\n",
    "    json_result = json.dumps(jsonData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/generalResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return jsonData  \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def sankeyFile(lda_model,numberOfTopics):\n",
    "    topicsData = lda_model.show_topics(num_topics = numberOfTopics,num_words=7,formatted=False)\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    for topic in topicsData:\n",
    "        currTopic = -1\n",
    "        for words in topic:\n",
    "            if type(words) == int:\n",
    "                currTopic = words\n",
    "            else:\n",
    "                for word, weight in words:\n",
    "                    sankeyRow = ['Tema '+str(currTopic+1),word,int(weight*100)]\n",
    "                    dataForSankey.append(sankeyRow)\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/sankeyResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------       \n",
    "def createDictForTopics(lda_model, numberOfTopics):\n",
    "    topicsDictWords = {}\n",
    "    i=1\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        for word, value in words:\n",
    "            listOfTopic.append(word)\n",
    "        topicsDictWords[i] = listOfTopic   \n",
    "        i+=1\n",
    "    return topicsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def createDictForOds(df):\n",
    "    odsDictWords = {}\n",
    "    for row_id, data in df.iterrows():\n",
    "        if data['ods'] not in odsDictWords:        \n",
    "            odsDictWords[data['ods']] = set()\n",
    "        if len(data['palabra'])>=3:\n",
    "            odsDictWords[data['ods']].add(data['palabra'])    \n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def simmilarityOdsandTopics(topicsDictWords, odsDictWords):\n",
    "    simmilarityTopics = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        lista = []\n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            num = len(set(list1) & set(list2))\n",
    "            lista.append({'ods': odsNum,'Similaridad': num})\n",
    "        datosTopico = {'topico':'Tema'+str(topicNum), 'SimilaridadOds' : lista}\n",
    "        simmilarityTopics.append(datosTopico)\n",
    "    return simmilarityTopics\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def frequentODS(simmilarity):\n",
    "    odsTopOrder = {}\n",
    "    for topic in simmilarity:\n",
    "        for data in topic['SimilaridadOds']:\n",
    "            if data['ods'] in odsTopOrder:\n",
    "                odsTopOrder[data['ods']] += int(data['Similaridad'])\n",
    "            else:\n",
    "                odsTopOrder[data['ods']] = int(data['Similaridad'])\n",
    "    odsTopOrder = list(odsTopOrder.items())\n",
    "    odsTopOrder.sort(key = lambda x: x[1], reverse = True)\n",
    "    #change format\n",
    "    ods = []\n",
    "    for numero, cantidad in odsTopOrder:\n",
    "        ods.append([numero, cantidad])\n",
    "    return ods\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights):\n",
    "    data_flat = [w for w_list in data_ready for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "    moreDataPointer = 0\n",
    "    i=1\n",
    "    json_data = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        histogram = [['Palabra','Frecuencia','Importancia']]\n",
    "        infoCompleta = []\n",
    "        for word, value in words:\n",
    "            word_count = counter[word]\n",
    "            listOfTopic.append({\"text\": word,\"value\":1})\n",
    "            infoCompleta.append({\"word\":word, 'Importancia':float(value*1000), 'Frecuencia':word_count})\n",
    "            histogram.append([word,word_count, float(value*1000)])\n",
    "        lista = []\n",
    "        for t in simmilarity:\n",
    "            if t['topico']=='Tema'+str(topic+1):\n",
    "                for data in t['SimilaridadOds']:\n",
    "                    lista.append([data['ods'],data['Similaridad']])\n",
    "        lista.sort(key = lambda x: x[1], reverse = True)\n",
    "        odsRelacionado = lista[0][0]\n",
    "        odsComplementario = lista[1][0]\n",
    "        json_data.append({\"name\":\"Tema \"+str(topic+1),'infoCompleta':infoCompleta,\"words\":listOfTopic,\"ods\":odsRelacionado, \"sexo\":moreInsights[moreDataPointer]['sexo'], 'edades': moreInsights[moreDataPointer]['edades'], \"odsComplementario\": odsComplementario,'histogram':histogram}) \n",
    "        moreDataPointer+=1\n",
    "        i+=1\n",
    "    json_result = json.dumps(json_data, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/dataPerTopic.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getChord(frequency,simmilarity):\n",
    "    frequency=frequency[:6]\n",
    "    names = []\n",
    "    dataForODS = {}\n",
    "    for ods, quatity in frequency:\n",
    "        names.append(ods)\n",
    "    for data in simmilarity:\n",
    "        for row in data['SimilaridadOds']:\n",
    "            if row['ods'] in names and row['Similaridad'] > 1:\n",
    "                if row['ods'] in dataForODS:\n",
    "                    dataForODS[row['ods']].append(data['topico'])\n",
    "                else:\n",
    "                    dataForODS[row['ods']] = [data['topico']]\n",
    "    chordData = []\n",
    "    nameData = []\n",
    "    for names, temas in dataForODS.items():\n",
    "        nameData.append('ODS'+str(names))\n",
    "        array = [int(temaname[temaname.find('a')+1:]) for temaname in temas]\n",
    "        if len(array) > 8: array = array[:8]\n",
    "        if len(array) < 8: array = array + ([0]*(8-len(array)))\n",
    "        chordData.append(array)\n",
    "    json_result = json.dumps(chordData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(nameData, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/chordNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def swarnData(lda_model,numberOfTopics):\n",
    "    topicsDictWords = []\n",
    "    i=1\n",
    "    swarnNames = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        swarnNames.append('Tema'+str(topic+1))\n",
    "        for word, value in words:\n",
    "            topicsDictWords.append({\"group\":'Tema'+str(topic+1),\"id\":word,\"value\":int(value*100),\"volume\":int(value*100)})\n",
    "        i+=1\n",
    "    json_result = json.dumps(swarnNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(topicsDictWords, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/swarnData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicODSWeight(simmilarity):\n",
    "    dataBars = []\n",
    "    keysNames = set()\n",
    "    for data in simmilarity:\n",
    "        odsResults = {'Temas': data['topico']}\n",
    "        for odsDetails in data['SimilaridadOds']:\n",
    "            keysNames.add(str(odsDetails['ods']))\n",
    "            odsResults[str(odsDetails['ods'])] = int(odsDetails['Similaridad'])\n",
    "        dataBars.append(odsResults)\n",
    "    keysNames = list(keysNames)\n",
    "    json_result = json.dumps(dataBars, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentage.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(keysNames, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/odsTopicPercentageKeys.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def limpiezaDiccionario(odsDictWords):\n",
    "    if 'animal' in odsDictWords[2]: odsDictWords[2].remove('animal')\n",
    "    if 'animales' in odsDictWords[2]: odsDictWords[2].remove('animales')\n",
    "    if 'clima' in odsDictWords[2]: odsDictWords[2].remove('clima')\n",
    "    if 'animales' in odsDictWords[2]: odsDictWords[2].remove('animales')\n",
    "    if 'cara' in odsDictWords[2]: odsDictWords[2].remove('cara')\n",
    "    if 'agua' in odsDictWords[5]: odsDictWords[5].remove('agua')\n",
    "    if 'asistencia' in odsDictWords[5]: odsDictWords[5].remove('asistencia')\n",
    "    if 'bono' in odsDictWords[5]: odsDictWords[5].remove('bono')\n",
    "    if 'animales' in odsDictWords[5]: odsDictWords[5].remove('animales')\n",
    "    odsDictWords[5].union(set(['violar','violación','violaciones','golpear','calmar','sexualidad','mujeres','lgtbi','aborto','mujer','embarazo','embarazos','violencia','violentar','violentar','violencia']))\n",
    "    if 'drogas' in odsDictWords[8]: odsDictWords[8].remove('drogas')\n",
    "    if 'ambiental' in odsDictWords[8]: odsDictWords[8].remove('ambiental')\n",
    "    if 'violencia' in odsDictWords[8]: odsDictWords[8].remove('violencia')\n",
    "    if 'perros' in odsDictWords[12]: odsDictWords[12].remove('perros')   \n",
    "    if 'perro' in odsDictWords[12]: odsDictWords[12].remove('perro') \n",
    "    odsDictWords[8].union(set(['animal','animales','perro','perros',]))\n",
    "    odsDictWords[8].union(set(['educación']))\n",
    "    odsDictWords[11].union(set(['parquear','vehiculo','vehiculos']))\n",
    "    odsDictWords[11].union(set(['ofreciendole','drogar','peligrar,venta,drogar,pegar']))\n",
    "    odsDictWords[16].union(set(['golpear','calmar','miedo','peligrar','violencia','violentar','violentar','violencia']))\n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def dominant():\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sexoSerie = df['sexo']\n",
    "        rangoEdadSerie = df['rangoEdad']\n",
    "        sent_topics_df = pd.concat([sent_topics_df, sexoSerie, rangoEdadSerie, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    \n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Sexo','rangoEdad','Text']\n",
    "    doc_lens = [['ODS','Length']]\n",
    "    for d in df_dominant_topic.Text:\n",
    "        if type(d)==list:\n",
    "            doc_lens.append([' ', len(d)])\n",
    "        else:\n",
    "            doc_lens.append([' ', 0])\n",
    "    json_result = json.dumps(doc_lens, default=json_util.default)\n",
    "    with open('../client/src/ModelResults/histogram.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return df_dominant_topic\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def moreTopicInsights(dominantData):\n",
    "    groupsByTopic = dominantData.groupby(['Dominant_Topic'])\n",
    "    topicData = []\n",
    "    i = 0\n",
    "    for row in groupsByTopic:\n",
    "        topic = 'Tema '+ str(i+1)\n",
    "        countsSexo = row[1]['Sexo'].value_counts()\n",
    "        countsRangoEdad = row[1]['rangoEdad'].value_counts()\n",
    "        countsEdad = []\n",
    "        for edad, cantidad in countsRangoEdad.iteritems():\n",
    "            countsEdad.append([edad,cantidad])\n",
    "        countsEdad.sort(key = lambda x: x[0])\n",
    "        sexos = []\n",
    "        for genero, cantidad in countsSexo.iteritems():\n",
    "            sexos.append({\"sexoNombre\": genero, \"value\": cantidad})\n",
    "        topicData.append({'topic':topic, 'sexo':sexos, 'edades': countsEdad})\n",
    "        i+=1\n",
    "    return topicData\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        print(new_doc[0])\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    numberOfTopics = 10\n",
    "    df = pd.read_pickle(\"./processedData.pkl\")\n",
    "    \n",
    "    data_ready = []\n",
    "    with open('data_ready.csv', newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            data_ready.append(row)\n",
    "    \n",
    "    lda_model, corpus, id2word = LDAModel(df,numberOfTopics,data_ready)\n",
    "    \n",
    "    topicsDictWords = createDictForTopics(lda_model, numberOfTopics)\n",
    "    odsDictWords = createDictForOds(df)\n",
    "    odsDictWords = limpiezaDiccionario(odsDictWords)\n",
    "    simmilarity = simmilarityOdsandTopics(topicsDictWords, odsDictWords)\n",
    "    frequency = frequentODS(simmilarity)\n",
    "    \n",
    "    dominantData = dominant()\n",
    "    moreInsights = moreTopicInsights(dominantData)\n",
    "    storeGeneralInsight(df, frequency)\n",
    "    sankeyFile(lda_model,numberOfTopics)\n",
    "    topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights)\n",
    "    getChord(frequency,simmilarity)\n",
    "    swarnData(lda_model,numberOfTopics)\n",
    "    topicODSWeight(simmilarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: {'acueductos',\n",
       "  'agua',\n",
       "  'aguas',\n",
       "  'basuras',\n",
       "  'bosque',\n",
       "  'contaminación',\n",
       "  'contaminada',\n",
       "  'higiene',\n",
       "  'lago',\n",
       "  'lavar',\n",
       "  'limpia',\n",
       "  'reciclaje',\n",
       "  'río',\n",
       "  'ríos',\n",
       "  'segura',\n",
       "  'seguro'},\n",
       " 3: {'abusan',\n",
       "  'accesibilidad',\n",
       "  'acceso',\n",
       "  'accidente',\n",
       "  'accidentes',\n",
       "  'adicción',\n",
       "  'agua',\n",
       "  'aguas',\n",
       "  'alucinógenas',\n",
       "  'alucinógenos',\n",
       "  'apoyar',\n",
       "  'asequible',\n",
       "  'cara',\n",
       "  'clima',\n",
       "  'cocinar',\n",
       "  'consumo',\n",
       "  'contaminación',\n",
       "  'contaminado',\n",
       "  'contaminando',\n",
       "  'contaminar',\n",
       "  'cuidado',\n",
       "  'curar',\n",
       "  'droga',\n",
       "  'drogadicción',\n",
       "  'drogadicto',\n",
       "  'drogas',\n",
       "  'edad',\n",
       "  'embarazo',\n",
       "  'embarazos',\n",
       "  'enfermedad',\n",
       "  'enfermedades',\n",
       "  'epidemia',\n",
       "  'estupefaciente',\n",
       "  'estupefacientes',\n",
       "  'familia',\n",
       "  'familias',\n",
       "  'hospital',\n",
       "  'infecciones',\n",
       "  'inseguridad',\n",
       "  'joven',\n",
       "  'jóvenes',\n",
       "  'madres',\n",
       "  'marihuana',\n",
       "  'marihuanero',\n",
       "  'medicamento',\n",
       "  'medicamentos',\n",
       "  'medicina',\n",
       "  'mejor',\n",
       "  'mercado',\n",
       "  'muerte',\n",
       "  'muertes',\n",
       "  'niños',\n",
       "  'país',\n",
       "  'polución',\n",
       "  'prevenir',\n",
       "  'salud',\n",
       "  'saludables',\n",
       "  'secundaria',\n",
       "  'sexual',\n",
       "  'sexualidad',\n",
       "  'sida',\n",
       "  'sustancia',\n",
       "  'tráfico',\n",
       "  'unión',\n",
       "  'vacuna',\n",
       "  'vacunas',\n",
       "  'vicioso',\n",
       "  'viviendas',\n",
       "  'vulnerables'},\n",
       " 11: {'acceso',\n",
       "  'acoso',\n",
       "  'agua',\n",
       "  'aire',\n",
       "  'alcantarillas',\n",
       "  'ambiente',\n",
       "  'ancianos',\n",
       "  'asfalto',\n",
       "  'automotor',\n",
       "  'barrio',\n",
       "  'basura',\n",
       "  'basuras',\n",
       "  'botadero',\n",
       "  'bus',\n",
       "  'calidad',\n",
       "  'calle',\n",
       "  'camino',\n",
       "  'caminos',\n",
       "  'campo',\n",
       "  'ciudad',\n",
       "  'ciudades',\n",
       "  'clima',\n",
       "  'comunidad',\n",
       "  'confianza',\n",
       "  'conflicto',\n",
       "  'construir',\n",
       "  'contaminación',\n",
       "  'cultural',\n",
       "  'daño',\n",
       "  'delictivos',\n",
       "  'delincuencia',\n",
       "  'delincuenciales',\n",
       "  'delincuente',\n",
       "  'delincuentes',\n",
       "  'delito',\n",
       "  'deporte',\n",
       "  'deportivo',\n",
       "  'deportivos',\n",
       "  'emergencia',\n",
       "  'esfuerzos',\n",
       "  'espacios',\n",
       "  'familia',\n",
       "  'gestión',\n",
       "  'humo',\n",
       "  'infraestructura',\n",
       "  'inseguridad',\n",
       "  'inundaciones',\n",
       "  'inundación',\n",
       "  'invierno',\n",
       "  'juventud',\n",
       "  'jóvenes',\n",
       "  'lluvias',\n",
       "  'metro',\n",
       "  'miedo',\n",
       "  'movilidad',\n",
       "  'municipio',\n",
       "  'natural',\n",
       "  'naturaleza',\n",
       "  'olores',\n",
       "  'parque',\n",
       "  'pasajero',\n",
       "  'pavimentación',\n",
       "  'peligro',\n",
       "  'peligroso',\n",
       "  'personas',\n",
       "  'pobre',\n",
       "  'pobreza',\n",
       "  'policía',\n",
       "  'preocuparse',\n",
       "  'proteger',\n",
       "  'pueblo',\n",
       "  'público',\n",
       "  'recreación',\n",
       "  'residuo',\n",
       "  'ruta',\n",
       "  'seguridad',\n",
       "  'semaforización',\n",
       "  'semáforo',\n",
       "  'semáforos',\n",
       "  'sociedad',\n",
       "  'subsidio',\n",
       "  'trancones',\n",
       "  'trancón',\n",
       "  'transporte',\n",
       "  'tráfico',\n",
       "  'tránsito',\n",
       "  'universidad',\n",
       "  'urbano',\n",
       "  'vandalismo',\n",
       "  'vecindario',\n",
       "  'velocidad',\n",
       "  'verde',\n",
       "  'verdes',\n",
       "  'vial',\n",
       "  'vidas',\n",
       "  'viejitos',\n",
       "  'viejos',\n",
       "  'violencia',\n",
       "  'vivienda',\n",
       "  'vándalos',\n",
       "  'vía',\n",
       "  'víctima',\n",
       "  'zona',\n",
       "  'zonas'},\n",
       " 1: {'adulto',\n",
       "  'agua',\n",
       "  'beneficios',\n",
       "  'bienes',\n",
       "  'campesinos',\n",
       "  'ciudad',\n",
       "  'comida',\n",
       "  'control',\n",
       "  'derecho',\n",
       "  'derechos',\n",
       "  'desastre',\n",
       "  'desempleados',\n",
       "  'dinero',\n",
       "  'discriminación',\n",
       "  'educación',\n",
       "  'embarazo',\n",
       "  'empleos',\n",
       "  'equidad',\n",
       "  'erradicar',\n",
       "  'familia',\n",
       "  'fenómenos',\n",
       "  'hambre',\n",
       "  'herencia',\n",
       "  'hogar',\n",
       "  'hogares',\n",
       "  'igualdad',\n",
       "  'ingresos',\n",
       "  'internet',\n",
       "  'país',\n",
       "  'pensión',\n",
       "  'personas',\n",
       "  'población',\n",
       "  'pobre',\n",
       "  'pobres',\n",
       "  'pobreza',\n",
       "  'programas',\n",
       "  'propiedad',\n",
       "  'recursos',\n",
       "  'recursos económicos',\n",
       "  'reducción',\n",
       "  'riesgo',\n",
       "  'salud',\n",
       "  'social',\n",
       "  'sufren',\n",
       "  'tierra',\n",
       "  'vivienda',\n",
       "  'viviendas',\n",
       "  'vulnerabilidad',\n",
       "  'vulnerable',\n",
       "  'vulnerables'},\n",
       " 8: {'ambiental',\n",
       "  'ciudad',\n",
       "  'comercio',\n",
       "  'desastres',\n",
       "  'desempleado',\n",
       "  'desempleados',\n",
       "  'desempleo',\n",
       "  'dinero',\n",
       "  'drogas',\n",
       "  'economía',\n",
       "  'empleado',\n",
       "  'empleo',\n",
       "  'empresa',\n",
       "  'estudio',\n",
       "  'explotación',\n",
       "  'falta de oportunidades',\n",
       "  'familia',\n",
       "  'ingreso',\n",
       "  'ingresos',\n",
       "  'jóvenes',\n",
       "  'ocupación',\n",
       "  'oportunidad',\n",
       "  'oportunidades',\n",
       "  'plata',\n",
       "  'salario',\n",
       "  'trabajador',\n",
       "  'trabajo',\n",
       "  'turismo',\n",
       "  'unión',\n",
       "  'violencia'},\n",
       " 4: {'académico',\n",
       "  'acceso',\n",
       "  'adulto',\n",
       "  'aprendizaje',\n",
       "  'biblioteca',\n",
       "  'bibliotecas',\n",
       "  'bienestar',\n",
       "  'bullying',\n",
       "  'colegio',\n",
       "  'cuidado',\n",
       "  'discapacidad',\n",
       "  'educación',\n",
       "  'educar',\n",
       "  'educativa',\n",
       "  'educativas',\n",
       "  'educativo',\n",
       "  'escolar',\n",
       "  'escolares',\n",
       "  'escuela',\n",
       "  'escuelas',\n",
       "  'estudiante',\n",
       "  'estudiantes',\n",
       "  'estudio',\n",
       "  'familia',\n",
       "  'hombre',\n",
       "  'hombres',\n",
       "  'infantil',\n",
       "  'joven',\n",
       "  'jovenes',\n",
       "  'jóvenes',\n",
       "  'país',\n",
       "  'países',\n",
       "  'personas',\n",
       "  'primaria',\n",
       "  'salud',\n",
       "  'secundaria',\n",
       "  'universidad',\n",
       "  'violencia'},\n",
       " 9: {'acceso',\n",
       "  'ambiental',\n",
       "  'apoyo',\n",
       "  'calidad',\n",
       "  'carretera',\n",
       "  'ciencia',\n",
       "  'comunicación',\n",
       "  'construcción',\n",
       "  'contaminación',\n",
       "  'convocatoria',\n",
       "  'económico',\n",
       "  'estadio',\n",
       "  'frontera',\n",
       "  'industria',\n",
       "  'industrial',\n",
       "  'industrias',\n",
       "  'información',\n",
       "  'ingreso',\n",
       "  'interno',\n",
       "  'investigación',\n",
       "  'limpia',\n",
       "  'pequeña',\n",
       "  'producto',\n",
       "  'promover',\n",
       "  'préstamo',\n",
       "  'puente',\n",
       "  'puerto',\n",
       "  'resiliente',\n",
       "  'rural',\n",
       "  'sostenible',\n",
       "  'transporte',\n",
       "  'urbana',\n",
       "  'vereda',\n",
       "  'veredas',\n",
       "  'vía',\n",
       "  'vías'},\n",
       " 16: {'administración',\n",
       "  'agua',\n",
       "  'ambiente',\n",
       "  'apoyo',\n",
       "  'armas',\n",
       "  'asesinar',\n",
       "  'asesinato',\n",
       "  'atracadores',\n",
       "  'atracados',\n",
       "  'atracan',\n",
       "  'atracar',\n",
       "  'atraco',\n",
       "  'atracos',\n",
       "  'atraquen',\n",
       "  'autoridad',\n",
       "  'bala',\n",
       "  'banda',\n",
       "  'bandas',\n",
       "  'basuras',\n",
       "  'calidad',\n",
       "  'ciudad',\n",
       "  'ciudades',\n",
       "  'clima',\n",
       "  'colegio',\n",
       "  'comida',\n",
       "  'comunidad',\n",
       "  'confianza',\n",
       "  'conflicto',\n",
       "  'contaminación',\n",
       "  'corrupción',\n",
       "  'cuidado',\n",
       "  'daño',\n",
       "  'delictivos',\n",
       "  'delincuencia',\n",
       "  'democracia',\n",
       "  'derecho',\n",
       "  'desconfianza',\n",
       "  'dinero',\n",
       "  'drogas',\n",
       "  'elecciones',\n",
       "  'elegir',\n",
       "  'escuela',\n",
       "  'estudiantes',\n",
       "  'hombres',\n",
       "  'hurto',\n",
       "  'infantil',\n",
       "  'inseguridad',\n",
       "  'instituciones',\n",
       "  'joven',\n",
       "  'justicia',\n",
       "  'jóvenes',\n",
       "  'ley',\n",
       "  'maltrato',\n",
       "  'matar',\n",
       "  'mercancías',\n",
       "  'miedo',\n",
       "  'movilidad',\n",
       "  'muerte',\n",
       "  'muertes',\n",
       "  'participa',\n",
       "  'participar',\n",
       "  'paz',\n",
       "  'país',\n",
       "  'plantas',\n",
       "  'población',\n",
       "  'policía',\n",
       "  'protección',\n",
       "  'quitar',\n",
       "  'representa',\n",
       "  'riesgo',\n",
       "  'roban',\n",
       "  'robando',\n",
       "  'robar',\n",
       "  'robaron',\n",
       "  'roben',\n",
       "  'robo',\n",
       "  'salud',\n",
       "  'segura',\n",
       "  'seguridad',\n",
       "  'seguro',\n",
       "  'tolerancia',\n",
       "  'tráfico',\n",
       "  'tránsito',\n",
       "  'universidad',\n",
       "  'vacuna',\n",
       "  'vacunas',\n",
       "  'vereda',\n",
       "  'vida',\n",
       "  'vigilancia',\n",
       "  'violencia'},\n",
       " 12: {'accidente',\n",
       "  'animales',\n",
       "  'basuras',\n",
       "  'bosque',\n",
       "  'campesinos',\n",
       "  'comunidad',\n",
       "  'cuidado',\n",
       "  'educación',\n",
       "  'impuesto',\n",
       "  'limpio',\n",
       "  'locales',\n",
       "  'mejor',\n",
       "  'movilidad',\n",
       "  'necesidad',\n",
       "  'perjudicial',\n",
       "  'perro',\n",
       "  'perros',\n",
       "  'policía',\n",
       "  'prevención',\n",
       "  'productos',\n",
       "  'quitar',\n",
       "  'reciclar',\n",
       "  'residuos',\n",
       "  'salud',\n",
       "  'suelo',\n",
       "  'árboles'},\n",
       " 5: {'agua',\n",
       "  'animales',\n",
       "  'asistencia',\n",
       "  'bono',\n",
       "  'discriminando',\n",
       "  'discriminar',\n",
       "  'edad',\n",
       "  'familia',\n",
       "  'gay',\n",
       "  'hogar',\n",
       "  'hombres',\n",
       "  'homofobia',\n",
       "  'inmigrante',\n",
       "  'ley',\n",
       "  'machismo',\n",
       "  'maltrato',\n",
       "  'sexual',\n",
       "  'sufren',\n",
       "  'valores',\n",
       "  'violación',\n",
       "  'violencia'},\n",
       " 2: {'alimentación',\n",
       "  'alimentar',\n",
       "  'alimento',\n",
       "  'alimentos',\n",
       "  'animal',\n",
       "  'animales',\n",
       "  'cara',\n",
       "  'clima',\n",
       "  'comida',\n",
       "  'conservación',\n",
       "  'ecosistema',\n",
       "  'embarazos',\n",
       "  'infancia',\n",
       "  'infraestructura',\n",
       "  'infraestructuras',\n",
       "  'inseguridad',\n",
       "  'inundaciones',\n",
       "  'inundación',\n",
       "  'medio ambiente',\n",
       "  'mercado',\n",
       "  'mercados',\n",
       "  'niños',\n",
       "  'nutrición',\n",
       "  'planta',\n",
       "  'plantas',\n",
       "  'precio',\n",
       "  'saludable',\n",
       "  'sana',\n",
       "  'tecnología'},\n",
       " 10: {'aumento',\n",
       "  'capital',\n",
       "  'clase',\n",
       "  'confianza',\n",
       "  'corriente',\n",
       "  'costo',\n",
       "  'crecimiento',\n",
       "  'cuidado',\n",
       "  'decisión',\n",
       "  'desigualdad',\n",
       "  'desplazamiento',\n",
       "  'económica',\n",
       "  'económicas',\n",
       "  'edad',\n",
       "  'entidades',\n",
       "  'especial',\n",
       "  'extranjero',\n",
       "  'fomentar',\n",
       "  'inclusión',\n",
       "  'inmigración',\n",
       "  'instituciones',\n",
       "  'intervenir',\n",
       "  'inversión',\n",
       "  'leyes',\n",
       "  'media',\n",
       "  'mejor',\n",
       "  'mejora',\n",
       "  'mejorar',\n",
       "  'migración',\n",
       "  'migratorio',\n",
       "  'movilidad',\n",
       "  'país',\n",
       "  'población',\n",
       "  'política',\n",
       "  'políticas',\n",
       "  'presupuesto',\n",
       "  'protección',\n",
       "  'racismo',\n",
       "  'responsable',\n",
       "  'rico',\n",
       "  'riqueza',\n",
       "  'social',\n",
       "  'sueldo',\n",
       "  'tratado',\n",
       "  'trato',\n",
       "  'venezolanos',\n",
       "  'venezuela',\n",
       "  'vida',\n",
       "  'vigilancia'},\n",
       " 7: {'alumbrado', 'corte', 'energía', 'gas', 'limpio', 'luz', 'servicio'},\n",
       " 13: {'calentamiento',\n",
       "  'clima',\n",
       "  'climático',\n",
       "  'deforestación',\n",
       "  'inundaciones',\n",
       "  'islas',\n",
       "  'preparar',\n",
       "  'verano'},\n",
       " 15: {'gato', 'gatos', 'parque', 'perro', 'perros', 'ríos', 'tala'},\n",
       " 17: {'internacional'},\n",
       " 14: {'isla', 'matar'}}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createDictForOds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.137*\"inseguridad,mujer,inseguridad\" + 0.120*\"violencia,mujer,violencia\" + 0.051*\"inseguridad\" + 0.020*\"inseguridad,robo,mujer,inseguridad\" + 0.018*\"drogar,mujer,drogar\" + 0.008*\"movilidad\" + 0.002*\"mujer,maltratar\" + 0.001*\"violencia,conflicto,violencia\" + 0.001*\"mascota,recoger,excremento,mujer\" + 0.000*\"padre,estrato,trabajar,sustentar,familia,padre,moral,psicologicos,mujer,familia\"'),\n",
       " (1,\n",
       "  '0.114*\"violencia,violencia\" + 0.025*\"consumir,drogar,drogar\" + 0.025*\"seguridad\" + 0.021*\"violencia,intrafamiliar,mujer,violencia\" + 0.019*\"oportunidad,jovenes,mujer,jovenes\" + 0.006*\"vias,mujer,vias\" + 0.003*\"ambiental,basura,mujer,basura\" + 0.003*\"oportunidad,laboral,mujer,oportunidad\" + 0.002*\"perjudicar\" + 0.002*\"oportunidad\"'),\n",
       " (2,\n",
       "  '0.100*\"basura,mujer,basura\" + 0.024*\"oportunidad,mujer,oportunidad\" + 0.020*\"ley,ley\" + 0.012*\"pobreza,pobreza\" + 0.005*\"drogar,jovenes,mujer,drogar\" + 0.004*\"robar,mujer,robar\" + 0.003*\"pobreza,desempleo,mujer,pobreza\" + 0.002*\"carretero,mujer,carretero\" + 0.002*\"basura,callar,basura\" + 0.002*\"muchisimo,mujer\"'),\n",
       " (3,\n",
       "  '0.054*\"oportunidad,emplear\" + 0.052*\"emplear\" + 0.027*\"consumir,drogar,mujer,drogar\" + 0.023*\"basura,basura,mujer,basura\" + 0.008*\"vias,acceso,acceso\" + 0.006*\"ambiental,ambiental\" + 0.005*\"banda\" + 0.005*\"robar\" + 0.004*\"delincuencial\" + 0.003*\"movilidad,vehicular,movilidad\"'),\n",
       " (4,\n",
       "  '0.043*\"mujer,seguridad\" + 0.027*\"delincuencia\" + 0.020*\"robo,mujer\" + 0.020*\"conflicto\" + 0.015*\"seguridad,seguridad\" + 0.012*\"seguridad,mujer,seguridad\" + 0.012*\"jovenes,jovenes\" + 0.009*\"banda,mujer,banda\" + 0.008*\"inseguridad,inseguridad,mujer,inseguridad\" + 0.008*\"desigualdad\"'),\n",
       " (5,\n",
       "  '0.060*\"desempleo,desempleo\" + 0.038*\"transportar,mujer,transportar\" + 0.037*\"movilidad,mujer,movilidad\" + 0.034*\"delincuencia,mujer,delincuencia\" + 0.025*\"basura\" + 0.023*\"inseguridad,mujer\" + 0.011*\"jovenes\" + 0.008*\"inseguridad,violencia,mujer,violencia\" + 0.008*\"delincuencia,delincuencia\" + 0.007*\"animal,mujer,animal\"'),\n",
       " (6,\n",
       "  '0.075*\"movilidad,movilidad\" + 0.026*\"mujer,inseguridad\" + 0.017*\"salud,mujer,salud\" + 0.009*\"mujer,publicar\" + 0.006*\"basura,tirar,mujer,basura\" + 0.003*\"desempleo,oportunidad,mujer,desempleo\" + 0.003*\"venta,drogar,mujer,drogar\" + 0.002*\"desempleo\" + 0.002*\"delincuencia,inseguridad,mujer,delincuencia\" + 0.001*\"consumir\"'),\n",
       " (7,\n",
       "  '0.032*\"pobreza,mujer,pobreza\" + 0.031*\"desempleo,desempleo,desempleo\" + 0.029*\"basura,basura\" + 0.026*\"inseguridad,violencia,violencia\" + 0.008*\"salud,salud\" + 0.008*\"desempleo,mujer,desempleo\" + 0.007*\"oportunidad,jovenes,jovenes\" + 0.004*\"publicos\" + 0.003*\"conflicto,mujer\" + 0.002*\"violencia,combo,mujer,violencia\"'),\n",
       " (8,\n",
       "  '0.006*\"perro,perro\" + 0.005*\"carro\" + 0.005*\"inseguridad,inseguridad,inseguridad\" + 0.003*\"educacion,jovenes,mujer,jovenes\" + 0.003*\"rios,rios\" + 0.002*\"robo,mujer,inseguridad\" + 0.002*\"menor\" + 0.002*\"afrontar,mujer\" + 0.001*\"inseguridad,mujer,inseguridad,abr\" + 0.001*\"seguridad,policia,mujer,seguridad\"'),\n",
       " (9,\n",
       "  '0.155*\"mujer\" + 0.112*\"inseguridad,inseguridad\" + 0.028*\"drogar,drogar\" + 0.021*\"mujer,salud\" + 0.012*\"emplear,emplear\" + 0.007*\"ambiental\" + 0.006*\"salud\" + 0.004*\"mujer,residuo\" + 0.003*\"tirar,basura\" + 0.002*\"drogar\"')]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
