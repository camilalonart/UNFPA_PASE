{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import argparse\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "import csv\n",
    "\n",
    "import spacy\n",
    "\n",
    "def LDAModel(df,numberOfTopics,data_ready):\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=numberOfTopics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=2000,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True)\n",
    "\n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def modelPerplexityCoherenceScore(lda_model,data_words_trigrams):\n",
    "    # Compute Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return perplexity, coherence_lda\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def storeGeneralInsight(df, odsData):\n",
    "    jsonData = {}\n",
    "    #ANIO--------------------------------------------------\n",
    "    jsonData['anio'] = [int(df.anio[0])]\n",
    "    \n",
    "    #RESPUESTAS POR PREGUNTA-----------------------------------------------\n",
    "    preguntasCount = df['pregunta'].value_counts()\n",
    "    cantidadDePreguntas = len(preguntasCount)\n",
    "    preguntasDict = {}\n",
    "    for preguntaP, cantidadP in preguntasCount.iteritems():\n",
    "        preguntasDict[preguntaP] = {'pregunta': preguntaP, 'cantidad': cantidadP}\n",
    "    preguntasListas = list(preguntasDict.values())\n",
    "    jsonData['preguntas'] = list(preguntasDict.values())\n",
    "    \n",
    "    #NUMERO DE RESPUESTAS--------------------------------------------------\n",
    "    total = 0\n",
    "    for p in preguntasListas:\n",
    "        total = max(total, p['cantidad'])\n",
    "    jsonData['totalRespuestas'] = total\n",
    "    \n",
    "    #RESPUESTAS POR EDAD-----------------------------------------------\n",
    "    edades = []\n",
    "    for edad, cantidad in df['rangoEdad'].value_counts().iteritems():\n",
    "        edades.append([edad,cantidad//cantidadDePreguntas])\n",
    "    edades.sort(key = lambda x: x[0])\n",
    "    jsonData['edad'] = edades\n",
    "    \n",
    "    #RESPUESTAS POR SEXO-----------------------------------------------\n",
    "    sexos = []\n",
    "    for genero, cantidad in df['sexo'].value_counts().iteritems():\n",
    "        sexoActual = {\"sexoNombre\": genero, \"value\": cantidad//cantidadDePreguntas}\n",
    "        sexos.append(sexoActual)\n",
    "    jsonData['sexo'] = sexos\n",
    "    \n",
    "    #RESPUESTAS POR ODS------------------------------------------------\n",
    "    \n",
    "    jsonData['porOds'] = odsData\n",
    "    #TOP DE PALABRAS----------------------------------------------------\n",
    "    topPalabras = []\n",
    "    for numero, cantidad in df['palabra'].value_counts().iteritems():\n",
    "        topPalabras.append([numero, cantidad])\n",
    "    jsonData['topPalabras'] = topPalabras[:8]\n",
    "    #CANTIDAD DE ODS POR MES--------------------------------------------\n",
    "    ''' ODS en x y una linea por mes donde y es la cantidad de ods\n",
    "        [['../images/SDGs/1.png',\n",
    "          1,\n",
    "          [1, 0],\n",
    "          [2, 0],\n",
    "          [3, 0],\n",
    "          [4, 18],\n",
    "          [5, 496],\n",
    "          [6, 234],\n",
    "          [7, 192],\n",
    "          [8, 22],\n",
    "          [9, 0],\n",
    "          [10, 0],\n",
    "          [11, 0],\n",
    "          [12, 0]],\n",
    "    '''\n",
    "    datosODS = {}\n",
    "    datosPorMes = df.groupby(\"mesTexo\")\n",
    "    for mes, datos in datosPorMes:\n",
    "        for ods, cantidad in datos['ods'].value_counts().iteritems():\n",
    "            if ods not in datosODS:\n",
    "                datosODS[ods] = {'ods':ods}\n",
    "            datosODS[ods][mes] = cantidad\n",
    "    for ods in datosODS:\n",
    "        for mes, _ in datosPorMes:\n",
    "            if mes not in datosODS[ods]:\n",
    "                datosODS[ods][mes] = 0\n",
    "\n",
    "    baseImage = '../images/SDGs/'\n",
    "    mesesTable = {\"Ene\":1,\"Feb\":2,\"Mar\":3,\"Abr\":4,\"May\":5,\"Jun\":6,\"Jul\":7,\"Aug\":8,\"Sep\":9,\"Oct\":10,\"Nov\":11,\"Dic\":12}\n",
    "    datosODS = list(datosODS.values())\n",
    "    datosODS.sort(key = lambda x: x['ods'])\n",
    "    datosEnLista = []\n",
    "    for entrada in datosODS:\n",
    "        listVersion = []\n",
    "        for key, val in entrada.items():\n",
    "            listVersion.append([key,val])\n",
    "        monthsAlreadyIn = set()\n",
    "        for i in range(1,len(listVersion)):\n",
    "            monthsAlreadyIn.add(mesesTable[listVersion[i][0]])\n",
    "            listVersion[i][0] = mesesTable[listVersion[i][0]]\n",
    "        for i in range(1,13):\n",
    "            if i not in monthsAlreadyIn:\n",
    "                listVersion.append([i,0])\n",
    "        odsS = listVersion[1:]   \n",
    "        odsS.sort(key = lambda x: x[0])\n",
    "        #listVersion[0][0] = baseImage + str(listVersion[0][1]) + '.png'\n",
    "        #listVersion = listVersion[0] + odsS\n",
    "        datosEnLista.append(listVersion)\n",
    "    jsonData['datosPorMes'] = datosEnLista\n",
    "\n",
    "    json_result = json.dumps(jsonData, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/generalResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return jsonData  \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def sankeyFile(lda_model,numberOfTopics):\n",
    "    topicsData = lda_model.show_topics(num_topics = numberOfTopics,num_words=4,formatted=False)\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    for topic in topicsData:\n",
    "        currTopic = -1\n",
    "        for words in topic:\n",
    "            if type(words) == int:\n",
    "                currTopic = words\n",
    "            else:\n",
    "                for word, weight in words:\n",
    "                    sankeyRow = ['Tema '+str(currTopic+1),word,int(weight*100)]\n",
    "                    dataForSankey.append(sankeyRow)\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/sankeyResult.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------       \n",
    "def createDictForTopics(lda_model, numberOfTopics):\n",
    "    topicsDictWords = {}\n",
    "    i=1\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        for word, value in words:\n",
    "            listOfTopic.append(word)\n",
    "        topicsDictWords[i] = listOfTopic   \n",
    "        i+=1\n",
    "    return topicsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def createDictForOds(df):\n",
    "    odsDictWords = {}\n",
    "    for row_id, data in df.iterrows():\n",
    "        if data['ods'] not in odsDictWords:        \n",
    "            odsDictWords[data['ods']] = set()\n",
    "        if len(data['palabra'])>=3:\n",
    "            odsDictWords[data['ods']].add(data['palabra'])    \n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def simmilarityOdsandTopics(topicsDictWords, odsDictWords):\n",
    "    simmilarityTopics = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        lista = []\n",
    "        \n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            num = len(set(list1) & set(list2))\n",
    "            lista.append({'ods': odsNum,'Similaridad': num})\n",
    "        datosTopico = {'topico':'Tema'+str(topicNum), 'SimilaridadOds' : lista, 'Palabras': topicWords}\n",
    "        simmilarityTopics.append(datosTopico)\n",
    "    return simmilarityTopics\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def frequentODS(simmilarity):\n",
    "    odsTopOrder = {}\n",
    "    for topic in simmilarity:\n",
    "        for data in topic['SimilaridadOds']:\n",
    "            if data['ods'] in odsTopOrder:\n",
    "                odsTopOrder[data['ods']] += int(data['Similaridad'])\n",
    "            else:\n",
    "                odsTopOrder[data['ods']] = int(data['Similaridad'])\n",
    "    odsTopOrder = list(odsTopOrder.items())\n",
    "    odsTopOrder.sort(key = lambda x: x[1], reverse = True)\n",
    "    #change format\n",
    "    ods = []\n",
    "    for numero, cantidad in odsTopOrder:\n",
    "        ods.append([numero, cantidad])\n",
    "    return ods\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights):\n",
    "    data_flat = [w for w_list in data_ready for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "    moreDataPointer = 0\n",
    "    i=1\n",
    "    json_data = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        listOfTopic = []\n",
    "        histogram = [['Palabra','Frecuencia','Importancia']]\n",
    "        infoCompleta = []\n",
    "        for word, value in words:\n",
    "            word_count = counter[word]\n",
    "            listOfTopic.append({\"text\": word,\"value\":1})\n",
    "            infoCompleta.append({\"word\":word, 'Importancia':float(value*2000), 'Frecuencia':word_count})\n",
    "            histogram.append([word,word_count, float(value*2000)])\n",
    "        lista = []\n",
    "        for t in simmilarity:\n",
    "            if t['topico']=='Tema'+str(topic+1):\n",
    "                for data in t['SimilaridadOds']:\n",
    "                    lista.append([data['ods'],data['Similaridad']])\n",
    "        lista.sort(key = lambda x: x[1], reverse = True)\n",
    "        odsRelacionado = lista[0][0]\n",
    "        odsComplementario = lista[1][0]\n",
    "        json_data.append({\"name\":\"Tema \"+str(topic+1),'infoCompleta':infoCompleta,\"words\":listOfTopic,\"ods\":odsRelacionado, \"sexo\":moreInsights[moreDataPointer]['sexo'], 'edades': moreInsights[moreDataPointer]['edades'], \"odsComplementario\": odsComplementario,'histogram':histogram}) \n",
    "        moreDataPointer+=1\n",
    "        i+=1\n",
    "    json_result = json.dumps(json_data, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/dataPerTopic.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def getChord(frequency,simmilarity):\n",
    "    frequency=frequency[:6]\n",
    "    names = []\n",
    "    dataForODS = {}\n",
    "    for ods, quatity in frequency:\n",
    "        names.append(ods)\n",
    "    for data in simmilarity:\n",
    "        for row in data['SimilaridadOds']:\n",
    "            if row['ods'] in names and row['Similaridad'] > 1:\n",
    "                if row['ods'] in dataForODS:\n",
    "                    dataForODS[row['ods']].append(data['topico'])\n",
    "                else:\n",
    "                    dataForODS[row['ods']] = [data['topico']]\n",
    "    chordData = []\n",
    "    nameData = []\n",
    "    for names, temas in dataForODS.items():\n",
    "        nameData.append('ODS'+str(names))\n",
    "        array = [int(temaname[temaname.find('a')+1:]) for temaname in temas]\n",
    "        if len(array) > 8: array = array[:8]\n",
    "        if len(array) < 8: array = array + ([0]*(8-len(array)))\n",
    "        chordData.append(array)\n",
    "    json_result = json.dumps(chordData, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/chordData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(nameData, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/chordNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def swarnData(lda_model,numberOfTopics):\n",
    "    topicsDictWords = []\n",
    "    i=1\n",
    "    swarnNames = []\n",
    "    for topic, words in lda_model.show_topics(formatted=False, num_topics=numberOfTopics):\n",
    "        swarnNames.append('Tema'+str(topic+1))\n",
    "        for word, value in words:\n",
    "            topicsDictWords.append({\"group\":'Tema'+str(topic+1),\"id\":word,\"value\":int(value*100),\"volume\":int(value*100)})\n",
    "        i+=1\n",
    "    json_result = json.dumps(swarnNames, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/swarnNames.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(topicsDictWords, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/swarnData.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def topicODSWeight(simmilarity):\n",
    "    dataBars = []\n",
    "    keysNames = set()\n",
    "    for data in simmilarity:\n",
    "        odsResults = {'Temas': data['topico']}\n",
    "        for odsDetails in data['SimilaridadOds']:\n",
    "            keysNames.add(str(odsDetails['ods']))\n",
    "            odsResults[str(odsDetails['ods'])] = int(odsDetails['Similaridad'])\n",
    "        dataBars.append(odsResults)\n",
    "    keysNames = list(keysNames)\n",
    "    json_result = json.dumps(dataBars, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/odsTopicPercentage.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    json_result = json.dumps(keysNames, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/odsTopicPercentageKeys.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def limpiezaDiccionario(odsDictWords):\n",
    "    if 'animal' in odsDictWords[2]: odsDictWords[2].remove('animal')\n",
    "    if 'animales' in odsDictWords[2]: odsDictWords[2].remove('animales')\n",
    "    if 'clima' in odsDictWords[2]: odsDictWords[2].remove('clima')\n",
    "    if 'animales' in odsDictWords[2]: odsDictWords[2].remove('animales')\n",
    "    if 'cara' in odsDictWords[2]: odsDictWords[2].remove('cara')\n",
    "    if 'agua' in odsDictWords[5]: odsDictWords[5].remove('agua')\n",
    "    if 'asistencia' in odsDictWords[5]: odsDictWords[5].remove('asistencia')\n",
    "    if 'bono' in odsDictWords[5]: odsDictWords[5].remove('bono')\n",
    "    if 'animales' in odsDictWords[5]: odsDictWords[5].remove('animales')\n",
    "    odsDictWords[5].union(set(['violar','violación','violaciones','golpear','robo,mujer','maltratar','calmar','sexualidad','mujeres','lgtbi','aborto','mujer','embarazo','embarazos','violencia','violentar','violentar','violencia']))\n",
    "    if 'drogas' in odsDictWords[8]: odsDictWords[8].remove('drogas')\n",
    "    if 'ambiental' in odsDictWords[8]: odsDictWords[8].remove('ambiental')\n",
    "    if 'violencia' in odsDictWords[8]: odsDictWords[8].remove('violencia')\n",
    "    if 'perros' in odsDictWords[12]: odsDictWords[12].remove('perros')   \n",
    "    if 'perro' in odsDictWords[12]: odsDictWords[12].remove('perro') \n",
    "    if 'empleo' in odsDictWords[11]: odsDictWords[11].remove('empleo') \n",
    "    if 'empleo' in odsDictWords[16]: odsDictWords[16].remove('empleo') \n",
    "    if 'empleo' in odsDictWords[3]: odsDictWords[3].remove('empleo')    \n",
    "    if 'desempleo' in odsDictWords[11]: odsDictWords[11].remove('desempleo') \n",
    "    if 'desempleo' in odsDictWords[16]: odsDictWords[16].remove('desempleo') \n",
    "    if 'desempleo' in odsDictWords[3]: odsDictWords[3].remove('desempleo')\n",
    "    odsDictWords[8].union(set(['animal','animales','perro','perros',]))\n",
    "    odsDictWords[8].union(set(['educación']))\n",
    "    odsDictWords[11].union(set(['parquear','vehiculo','vehiculos']))\n",
    "    odsDictWords[11].union(set(['ofreciendole','drogar','peligrar,venta,drogar,pegar']))\n",
    "    odsDictWords[16].union(set(['golpear','calmar','miedo','peligrar','violencia','violentar','violentar','violencia']))\n",
    "    return odsDictWords\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def dominant():\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            \n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sexoSerie = df['sexo']\n",
    "        rangoEdadSerie = df['rangoEdad']\n",
    "        sent_topics_df = pd.concat([sent_topics_df, sexoSerie, rangoEdadSerie, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "    df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    \n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Sexo','rangoEdad','Text']\n",
    "    doc_lens = [['ODS','Length']]\n",
    "    for d in df_dominant_topic.Text:\n",
    "        if type(d)==list:\n",
    "            doc_lens.append([' ', len(d)])\n",
    "        else:\n",
    "            doc_lens.append([' ', 0])\n",
    "    json_result = json.dumps(doc_lens, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/histogram.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "    return df_dominant_topic\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def moreTopicInsights(dominantData):\n",
    "    groupsByTopic = dominantData.groupby(['Dominant_Topic'])\n",
    "    generos = df['sexo'].value_counts()\n",
    "    mujerCount = generos[0]\n",
    "    hombreCount = generos[1]\n",
    "    print(mujerCount,hombreCount)\n",
    "    topicData = []\n",
    "    i = 0\n",
    "    for row in groupsByTopic:\n",
    "        topic = 'Tema '+ str(i+1)\n",
    "        countsSexo = row[1]['Sexo'].value_counts()\n",
    "        countsRangoEdad = row[1]['rangoEdad'].value_counts()\n",
    "        countsEdad = []\n",
    "        for edad, cantidad in countsRangoEdad.iteritems():\n",
    "            countsEdad.append([edad,cantidad])\n",
    "        countsEdad.sort(key = lambda x: x[0])\n",
    "        sexos = []\n",
    "        for genero, cantidad in countsSexo.iteritems():\n",
    "            if genero == 'Mujer':\n",
    "                sexos.append({\"sexoNombre\": genero, \"value\": int((cantidad/mujerCount)*10000)})\n",
    "            elif genero == 'Hombre':\n",
    "                sexos.append({\"sexoNombre\": genero, \"value\": int((cantidad/hombreCount)*10000)})\n",
    "        topicData.append({'topic':topic, 'sexo':sexos, 'edades': countsEdad})\n",
    "        i+=1\n",
    "        \n",
    "    return topicData\n",
    "#-----------------------------------------------------------------------------------------------------   \n",
    "def sankeyFileODS(odsDictWords,topicsDictWords):\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    lista = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            union = set(list1) & set(list2)\n",
    "            if 0 < len(union):\n",
    "                dataForSankey.append([\"ODS\"+str(odsNum),\"Tema\"+str(topicNum),3])\n",
    "                for word in union:\n",
    "                    dataForSankey.append([\"Tema\"+str(topicNum),word,1])\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/sankeyODS.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------     \n",
    "def encontrarODSTopico(lda_model,id2word):\n",
    "    def topicsWithNewQueriesODS(new_doc, ldamodel):\n",
    "        new_doc = process(new_doc)\n",
    "        for texts in new_doc[0]:\n",
    "            for text in texts:\n",
    "                print(text)\n",
    "            new_doc_bow = [id2word.doc2bow(texts)]\n",
    "            topics = ldamodel.get_document_topics(new_doc_bow)\n",
    "            for probabilities in topics:\n",
    "                probabilities.sort(key = lambda x : x[1], reverse=True)\n",
    "                print(lda_model.print_topic(probabilities[0][0]))\n",
    "                print(probabilities)\n",
    "    def relaciónDeOds(lda_model,id2word):\n",
    "        new_doc = {'respuesta':['erradicar la pobreza extrema para todas las personas en el mundo','La delincuencia y drogadicción son un problema']}\n",
    "        df = pd.DataFrame(new_doc)\n",
    "        topicsWithNewQueriesODS(df, lda_model)\n",
    "    relaciónDeOds(lda_model,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo construido\n",
      "Relación con ODSs Lista\n",
      "Dominantes Listo\n",
      "4234 3440\n",
      "Datos en front\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    numberOfTopics = 10\n",
    "    df = pd.read_pickle(\"./processedData.pkl\")\n",
    "    \n",
    "    new_data = []\n",
    "    with open('data_ready.csv', newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            new_data.append(row)\n",
    "    data_ready = []\n",
    "    for d in new_data:\n",
    "        for a in d:\n",
    "            data_ready.append(a.split(','))\n",
    "    lda_model, corpus, id2word = LDAModel(df,numberOfTopics,data_ready)\n",
    "    print(\"Modelo construido\")\n",
    "    topicsDictWords = createDictForTopics(lda_model, numberOfTopics)\n",
    "    odsDictWords = createDictForOds(df)\n",
    "    odsDictWords = limpiezaDiccionario(odsDictWords)\n",
    "    simmilarity = simmilarityOdsandTopics(topicsDictWords, odsDictWords)\n",
    "    frequency = frequentODS(simmilarity)\n",
    "    print(\"Relación con ODSs Lista\")\n",
    "    dominantData = dominant()\n",
    "    print(\"Dominantes Listo\")\n",
    "    moreInsights = moreTopicInsights(dominantData)\n",
    "    storeGeneralInsight(df, frequency)\n",
    "    sankeyFile(lda_model,numberOfTopics)\n",
    "    topicData(lda_model,numberOfTopics,simmilarity, data_ready, moreInsights)\n",
    "    getChord(frequency,simmilarity)\n",
    "    swarnData(lda_model,numberOfTopics)\n",
    "    topicODSWeight(simmilarity)\n",
    "    sankeyFileODS(odsDictWords,topicsDictWords)\n",
    "    print(\"Datos en front\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankeyFileODS(odsDictWords,topicsDictWords):\n",
    "    dataForSankey = [['From', 'To', 'Weight']]\n",
    "    lista = []\n",
    "    for topicNum, topicWords in topicsDictWords.items(): \n",
    "        for odsNum, odsWords in odsDictWords.items():\n",
    "            list1 = odsWords\n",
    "            list2 = topicWords\n",
    "            union = set(list1) & set(list2)\n",
    "            if 0 < len(union):\n",
    "                dataForSankey.append([\"ODS\"+str(odsNum),\"Tema\"+str(topicNum),3])\n",
    "                for word in union:\n",
    "                    dataForSankey.append([\"Tema\"+str(topicNum),word,1])\n",
    "    json_result = json.dumps(dataForSankey, default=json_util.default)\n",
    "    with open('../frontend/src/ModelResults/sankeyODS.json', \"w\") as outfile:\n",
    "        outfile.write(json_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankeyFileODS(odsDictWords,topicsDictWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
